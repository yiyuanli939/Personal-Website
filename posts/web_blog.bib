@misc{17DataValidation,
  title = {17~ {{R Data Validation}}: Pointblank -- {{Reproducible}} and {{Trustworthy Workflows}} for {{Data Science}}},
  urldate = {2025-11-29},
  howpublished = {https://ubc-dsci.github.io/reproducible-and-trustworthy-workflows-for-data-science/lectures/131-data\_validation-r-pointblank.html},
  file = {/Users/yiyuanli/Zotero/storage/GGLZ9DH8/131-data_validation-r-pointblank.html}
}

@misc{ackermanStatisticalMultimetricEvaluation2025,
  title = {Statistical Multi-Metric Evaluation and Visualization of {{LLM}} System Predictive Performance},
  author = {Ackerman, Samuel and Farchi, Eitan and Raz, Orna and Toledo, Assaf},
  year = 2025,
  month = jan,
  number = {arXiv:2501.18243},
  eprint = {2501.18243},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.18243},
  urldate = {2025-10-05},
  abstract = {The evaluation of generative or discriminative large language model (LLM)-based systems is often a complex multi-dimensional problem. Typically, a set of system configuration alternatives are evaluated on one or more benchmark datasets, each with one or more evaluation metrics, which may differ between datasets. We often want to evaluate---with a statistical measure of significance---whether systems perform differently either on a given dataset according to a single metric, on aggregate across metrics on a dataset, or across datasets. Such evaluations can be done to support decision-making, such as deciding whether a particular system component change (e.g., choice of LLM or hyperparameter values) significantly improves performance over the current system configuration, or, more generally, whether a fixed set of system configurations (e.g., a leaderboard list) have significantly different performances according to metrics of interest. We present a framework implementation that automatically performs the correct statistical tests, properly aggregates the statistical results across metrics and datasets (a nontrivial task), and can visualize the results. The framework is demonstrated on the multi-lingual code generation benchmark CrossCodeEval, for several state-of-the-art LLMs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Applications},
  file = {/Users/yiyuanli/Zotero/storage/V68YHQLZ/Ackerman et al. - 2025 - Statistical multi-metric evaluation and visualization of LLM system predictive performance.pdf}
}

@article{albadriExpressionBarhl1aReporter2020,
  title = {Expression of a {{Barhl1a}} Reporter in Subsets of Retinal Ganglion Cells and Commissural Neurons of the Developing Zebrafish Brain},
  author = {Albadri, Shahad and Armant, Olivier and {Aljand-Geschwill}, Tairi and Del Bene, Filippo and Carl, Matthias and Str{\"a}hle, Uwe and Poggi, Lucia},
  year = 2020,
  month = jun,
  journal = {Scientific Reports},
  volume = {10},
  number = {1},
  pages = {8814},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-65435-w},
  urldate = {2025-10-08},
  abstract = {Promoting the regeneration or survival of retinal ganglion cells (RGCs) is one focus of regenerative medicine. Homeobox Barhl transcription factors might be instrumental in these processes. In mammals, only barhl2 is expressed in the retina and is required for both subtype identity acquisition of amacrine cells and for the survival of RGCs downstream of Atoh7, a transcription factor necessary for RGC genesis. The underlying mechanisms of this dual role of Barhl2 in mammals have remained elusive. Whole genome duplication in the teleost lineage generated the barhl1a and barhl2 paralogues. In the Zebrafish retina, Barhl2 functions as a determinant of subsets of amacrine cells lineally related to RGCs independently of Atoh7. In contrast, barhl1a expression depends on Atoh7 but its expression dynamics and function have not been studied. Here we describe for the first time a Barhl1a reporter line in vivo showing that barhl1a turns on exclusively in subsets of RGCs and their post-mitotic precursors. We also show transient expression of barhl1a:GFP~in diencephalic neurons extending their axonal projections as part of the post-optic commissure, at the time of optic chiasm formation. This work sets the ground for future studies on RGC subtype identity, axonal projections and genetic specification of Barhl1a-positive RGCs and commissural neurons.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {Cell death in the nervous system,Developmental neurogenesis,Differentiation,Neural stem cells,Reprogramming},
  file = {/Users/yiyuanli/Zotero/storage/G432XPH6/Albadri et al. - 2020 - Expression of a Barhl1a reporter in subsets of retinal ganglion cells and commissural neurons of the.pdf}
}

@article{ApproachesStateAlternative2021,
  title = {Approaches to the {{State}}: {{Alternative Conceptions}} and {{Historical Dynamics}}},
  year = 2021,
  pages = {25},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/QD85FKY6/2021 - Approaches to the State Alternative Conceptions a.pdf}
}

@article{architSegmentAnythingMicroscopy2025,
  title = {Segment {{Anything}} for {{Microscopy}}},
  author = {Archit, Anwai and Freckmann, Luca and Nair, Sushmita and Khalid, Nabeel and Hilt, Paul and Rajashekar, Vikas and Freitag, Marei and Teuber, Carolin and Spitzner, Melanie and Tapia Contreras, Constanza and Buckley, Genevieve and {von Haaren}, Sebastian and Gupta, Sagnik and Grade, Marian and Wirth, Matthias and Schneider, G{\"u}nter and Dengel, Andreas and Ahmed, Sheraz and Pape, Constantin},
  year = 2025,
  month = mar,
  journal = {Nature Methods},
  volume = {22},
  number = {3},
  pages = {579--591},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-024-02580-4},
  urldate = {2025-09-29},
  abstract = {Accurate segmentation of objects in microscopy images remains a bottleneck for many researchers despite the number of tools developed for this purpose. Here, we present Segment Anything for Microscopy ({$\mu$}SAM), a tool for segmentation and tracking in multidimensional microscopy data. It is based on Segment Anything, a vision foundation model for image segmentation. We extend it by fine-tuning generalist models for light and electron microscopy that clearly improve segmentation quality for a wide range of imaging conditions. We also implement interactive and automatic segmentation in a napari plugin that can speed up diverse segmentation tasks and provides a unified solution for microscopy annotation across different microscopy modalities. Our work constitutes the application of vision foundation models in microscopy, laying the groundwork for solving image analysis tasks in this domain with a small set of powerful deep learning models.},
  copyright = {2025 The Author(s)},
  langid = {english},
  keywords = {Image processing,Software},
  file = {/Users/yiyuanli/Zotero/storage/LD2BEVNU/Archit et al. - 2025 - Segment Anything for Microscopy.pdf}
}

@misc{atilNonDeterminismDeterministicLLM2025,
  title = {Non-{{Determinism}} of "{{Deterministic}}" {{LLM Settings}}},
  author = {Atil, Berk and Aykent, Sarp and Chittams, Alexa and Fu, Lisheng and Passonneau, Rebecca J. and Radcliffe, Evan and Rajagopal, Guru Rajan and Sloan, Adam and Tudrej, Tomasz and Ture, Ferhan and Wu, Zhe and Xu, Lixinyu and Baldwin, Breck},
  year = 2025,
  month = apr,
  number = {arXiv:2408.04667},
  eprint = {2408.04667},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.04667},
  urldate = {2025-09-29},
  abstract = {LLM (large language model) practitioners commonly notice that outputs can vary for the same inputs, but we have been unable to find work that evaluates LLM stability as the main objective. In our study of 6 deterministically configured LLMs across 8 common tasks with 5 identical runs, we see accuracy variations up to 10\%. In addition, no LLM consistently delivers repeatable accuracy across all tasks. We also show examples of variation that are not normally distributed and compare configurations with zero-shot/few-shot prompting and fine-tuned examples. To better quantify what is going on, we introduce metrics focused on stability: TARr@N for the total agreement rate at N runs over raw output, and TARa@N for total agreement over parsed-out answers. We suggest that stability metrics be integrated into leader boards and research results going forward.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {/Users/yiyuanli/Zotero/storage/K8XYX9I2/Atil et al. - 2025 - Non-Determinism of Deterministic LLM Settings.pdf}
}

@misc{baktashGpt4ReviewAdvancements2023,
  title = {Gpt-4: {{A Review}} on {{Advancements}} and {{Opportunities}} in {{Natural Language Processing}}},
  shorttitle = {Gpt-4},
  author = {Baktash, Jawid Ahmad and Dawodi, Mursal},
  year = 2023,
  month = may,
  number = {arXiv:2305.03195},
  eprint = {2305.03195},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.03195},
  urldate = {2025-12-18},
  abstract = {Generative Pre-trained Transformer 4 (GPT-4) is the fourth-generation language model in the GPT series, developed by OpenAI, which promises significant advancements in the field of natural language processing (NLP). In this research article, we have discussed the features of GPT-4, its potential applications, and the challenges that it might face. We have also compared GPT-4 with its predecessor, GPT-3. GPT-4 has a larger model size (more than one trillion), better multilingual capabilities, improved contextual understanding, and reasoning capabilities than GPT-3. Some of the potential applications of GPT-4 include chatbots, personal assistants, language translation, text summarization, and question-answering. However, GPT-4 poses several challenges and limitations such as computational requirements, data requirements, and ethical concerns.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {/Users/yiyuanli/Zotero/storage/AF74NZ2X/Baktash and Dawodi - 2023 - Gpt-4 A Review on Advancements and Opportunities in Natural Language Processing.pdf;/Users/yiyuanli/Zotero/storage/N6CXNNHM/2305.html}
}

@misc{bendinelliExploringLLMAgents2025,
  title = {Exploring {{LLM Agents}} for {{Cleaning Tabular Machine Learning Datasets}}},
  author = {Bendinelli, Tommaso and Dox, Artur and Holz, Christian},
  year = 2025,
  month = mar,
  number = {arXiv:2503.06664},
  eprint = {2503.06664},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.06664},
  urldate = {2025-11-17},
  abstract = {High-quality, error-free datasets are a key ingredient in building reliable, accurate, and unbiased machine learning (ML) models. However, real world datasets often suffer from errors due to sensor malfunctions, data entry mistakes, or improper data integration across multiple sources that can severely degrade model performance. Detecting and correcting these issues typically require tailor-made solutions and demand extensive domain expertise. Consequently, automation is challenging, rendering the process labor-intensive and tedious. In this study, we investigate whether Large Language Models (LLMs) can help alleviate the burden of manual data cleaning. We set up an experiment in which an LLM, paired with Python, is tasked with cleaning the training dataset to improve the performance of a learning algorithm without having the ability to modify the training pipeline or perform any feature engineering. We run this experiment on multiple Kaggle datasets that have been intentionally corrupted with errors. Our results show that LLMs can identify and correct erroneous entries---such as illogical values or outliers---by leveraging contextual information from other features within the same row, as well as feedback from previous iterations. However, they struggle to detect more complex errors that require understanding data distribution across multiple rows, such as trends and biases.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/yiyuanli/Zotero/storage/78V26C8H/Bendinelli et al. - 2025 - Exploring LLM Agents for Cleaning Tabular Machine Learning Datasets.pdf}
}

@article{bengioNeuralProbabilisticLanguage,
  title = {A {{Neural Probabilistic Language Model}}},
  author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
  abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/JXSQEBSD/Bengio et al. - A Neural Probabilistic Language Model.pdf}
}

@article{bermanCivilSocietyCollapse1997,
  title = {Civil {{Society}} and the {{Collapse}} of the {{Weimar Republic}}},
  author = {Berman, Sheri},
  year = 1997,
  month = apr,
  journal = {World Politics},
  volume = {49},
  number = {3},
  pages = {401--429},
  issn = {0043-8871, 1086-3338},
  doi = {10.1353/wp.1997.0008},
  urldate = {2021-09-11},
  abstract = {Practically everywhere one looks these days the concept of ``civil society'' is in vogue. Neo-Tocquevillean scholars argue that civil society plays a role in driving political, social, and even economic outcomes. This new conventional wisdom, however, is flawed. It is simply not true that democratic government is always strengthened, not weakened, when it faces a vigorous civil society. This essay shows how a robust civil society helped scuttle the twentieth century's most critical democratic experiment, Weimar Germany. An important implication of this analysis is that under certain circumstances associationism and the prospects for democratic stability can actually be inversely related. To know when civil society activity will take on oppositional or even antidemocratic tendencies, one needs to ground one's analyses in concrete examinations of political reality. Political scientists should remember that Tocqueville considered Americans' political associations to be as important as their nonpolitical ones, and they should therefore examine more closely the connections between the two under various conditions.},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/CQAUETKQ/Berman - 1997 - Civil Society and the Collapse of the Weimar Repub.pdf}
}

@misc{BibliothekFurWissenschaftsgeschichte,
  title = {Bibliothek F\"ur {{Wissenschaftsgeschichte}}},
  journal = {Bibliothek~f\"ur~Wissenschaftsgeschichte},
  urldate = {2021-05-11},
  abstract = {a public Zotero research library for history of science and knowledge},
  copyright = {All rights reserved},
  howpublished = {https://wissenschaftsgeschichte.wordpress.com/},
  langid = {english}
}

@book{blitzsteinIntroductionProbability2015,
  title = {Introduction to Probability},
  author = {Blitzstein, Joseph K.},
  year = 2015,
  series = {Texts in Statistical Science},
  edition = {1st edition},
  publisher = {CRC Press/Taylor \& Francis Group},
  address = {Boca Raton},
  abstract = {"A Chapman \& Hall book.", Includes bibliographical references (pages 569-570) and index.},
  collaborator = {Hwang, Jessica},
  isbn = {978-1-4665-7559-2},
  keywords = {Probabilities,Textbooks}
}

@book{brownAmericanConstitutionalTradition2017,
  title = {The {{American}} Constitutional Tradition: Colonial Charters, Covenants, and Revolutionary State Constitutions, 1578-1780},
  shorttitle = {The {{American}} Constitutional Tradition},
  author = {Brown, H. Lowell},
  year = 2017,
  series = {The {{Fairleigh Dickinson University Press}} Law, Culture, and the Humanities Series},
  publisher = {The Farleigh Dickinson University Press},
  address = {Lanham, Maryland},
  abstract = {Series statement: The Fairleigh Dickinson University Press law, culture, and the humanities series, Includes bibliographical references and index.},
  isbn = {978-1-68393-049-5},
  langid = {english},
  keywords = {Constitutional law,History,States,United States}
}

@misc{buchholzDenoiSegJointDenoising2020,
  title = {{{DenoiSeg}}: {{Joint Denoising}} and {{Segmentation}}},
  shorttitle = {{{DenoiSeg}}},
  author = {Buchholz, Tim-Oliver and Prakash, Mangal and Krull, Alexander and Jug, Florian},
  year = 2020,
  month = jun,
  number = {arXiv:2005.02987},
  eprint = {2005.02987},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2005.02987},
  urldate = {2025-10-08},
  abstract = {Microscopy image analysis often requires the segmentation of objects, but training data for this task is typically scarce and hard to obtain. Here we propose DenoiSeg, a new method that can be trained end-to-end on only a few annotated ground truth segmentations. We achieve this by extending Noise2Void [10], a self-supervised denoising scheme that can be trained on noisy images alone, to also predict dense 3-class segmentations. The reason for the success of our method is that segmentation can profit from denoising, especially when performed jointly within the same network. The network becomes a denoising expert by seeing all available raw data, while co-learning to segment, even if only a few segmentation labels are available. This hypothesis is additionally fueled by our observation that the best segmentation results on high quality (very low noise) raw data are obtained when moderate amounts of synthetic noise are added. This renders the denoising-task non-trivial and unleashes the desired co-learning effect. We believe that DenoiSeg offers a viable way to circumvent the tremendous hunger for high quality training data and effectively enables few-shot learning of dense segmentations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/yiyuanli/Zotero/storage/TJMGS8IB/Buchholz et al. - 2020 - DenoiSeg Joint Denoising and Segmentation.pdf}
}

@article{caiSurveyMixtureExperts2025,
  title = {A {{Survey}} on {{Mixture}} of {{Experts}} in {{Large Language Models}}},
  author = {Cai, Weilin and Jiang, Juyong and Wang, Fan and Tang, Jing and Kim, Sunghun and Huang, Jiayi},
  year = 2025,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  eprint = {2407.06204},
  primaryclass = {cs},
  pages = {1--20},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10.1109/TKDE.2025.3554028},
  urldate = {2025-12-18},
  abstract = {Large language models (LLMs) have garnered unprecedented advancements across diverse fields, ranging from natural language processing to computer vision and beyond. The prowess of LLMs is underpinned by their substantial model size, extensive and diverse datasets, and the vast computational power harnessed during training, all of which contribute to the emergent abilities of LLMs (e.g., in-context learning) that are not present in small models. Within this context, the mixture of experts (MoE) has emerged as an effective method for substantially scaling up model capacity with minimal computation overhead, gaining significant attention from academia and industry. Despite its growing prevalence, there lacks a systematic and comprehensive review of the literature on MoE. This survey seeks to bridge that gap, serving as an essential resource for researchers delving into the intricacies of MoE. We first briefly introduce the structure of the MoE layer, followed by proposing a new taxonomy of MoE. Next, we overview the core designs for various MoE models including both algorithmic and systemic aspects, alongside collections of available open-source implementations, hyperparameter configurations and empirical evaluations. Furthermore, we delineate the multifaceted applications of MoE in practice, and outline some potential directions for future research. To facilitate ongoing updates and the sharing of cutting-edge advances in MoE research, we have established a resource repository at https://github.com/withinmiaov/A-Survey-on-Mixture-of-Experts-in-LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/yiyuanli/Zotero/storage/NJMV9RNJ/Cai et al. - 2025 - A Survey on Mixture of Experts in Large Language Models.pdf;/Users/yiyuanli/Zotero/storage/5I4U6W4R/2407.html}
}

@misc{caoHowShouldWe2025,
  title = {How {{Should We Build A Benchmark}}? {{Revisiting}} 274 {{Code-Related Benchmarks For LLMs}}},
  shorttitle = {How {{Should We Build A Benchmark}}?},
  author = {Cao, Jialun and Chan, Yuk-Kit and Ling, Zixuan and Wang, Wenxuan and Li, Shuqing and Liu, Mingwei and Qiao, Ruixi and Han, Yuting and Wang, Chaozheng and Yu, Boxi and He, Pinjia and Wang, Shuai and Zheng, Zibin and Lyu, Michael R. and Cheung, Shing-Chi},
  year = 2025,
  month = feb,
  number = {arXiv:2501.10711},
  eprint = {2501.10711},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.10711},
  urldate = {2025-10-04},
  abstract = {Various benchmarks have been proposed to assess the performance of large language models (LLMs) in different coding scenarios. We refer to them as code-related benchmarks. However, there are no systematic guidelines by which such a benchmark should be developed to assure its quality, reliability, and reproducibility. We propose How2Bench comprising a 55criteria checklist as a set of guidelines to comprehensively govern the development of coderelated benchmarks. Using HOW2BENCH, we profiled 274 benchmarks released within the past decade and found concerning issues. Nearly 70\% of the benchmarks did not take measures for data quality assurance; over 10\% did not even open source or only partially open source. Many highly cited benchmarks have loopholes, including duplicated samples, incorrect reference codes/tests/prompts, and unremoved sensitive/confidential information. Finally, we conducted a human study involving 49 participants and revealed significant gaps in awareness of the importance of data quality, reproducibility, and transparency. For ease of use, we provide a printable version of HOW2BENCH in Appendix E.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Software Engineering},
  file = {/Users/yiyuanli/Zotero/storage/EK7SHUHP/Cao et al. - 2025 - How Should We Build A Benchmark Revisiting 274 Code-Related Benchmarks For LLMs.pdf}
}

@misc{caoZengPingJia1925,
  title = {{Zeng ping jia zhu quan tu Hong lou meng: [120 hui]}},
  shorttitle = {{Zeng ping jia zhu quan tu Hong lou meng}},
  author = {Cao, Xueqin and {曹雪芹}},
  year = 1925,
  journal = {Zeng ping jia zhu quan tu Hong lou meng : [120 hui]},
  edition = {Tong wen shu ju cang ban., 同文書局藏版.},
  publisher = {Tong wen shu ju},
  address = {Shanghai},
  abstract = {Reproduction of [Guangxu] ji chou [1889] Shanghai tong wen shu ju cang ban., Reproduction of [光绪]己丑[1889]上海同文書局藏版.},
  collaborator = {Gao, E. and {高鄂}},
  langid = {chi}
}

@article{CauseSearchIts2021,
  title = {A {{Cause}} in {{Search}} of {{Its Effect}}, or {{What Does Political Culture Explain}}?},
  year = 2021,
  pages = {20},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/VIPZRNXF/2021 - A Cause in Search of Its Effect, or What Does Poli.pdf}
}

@misc{cetinkaya-rundelMinecetinkayarundelQuartotdg2025,
  title = {Mine-Cetinkaya-Rundel/Quarto-Tdg},
  author = {{Cetinkaya-Rundel}, Mine and Wickham, Charlotte},
  year = 2025,
  month = oct,
  urldate = {2025-10-27},
  abstract = {Quarto: The Definitive Guide}
}

@article{chatterjiHowPeopleUse,
  title = {How {{People Use ChatGPT}}},
  author = {Chatterji, Aaron and Cunningham, Thomas and Deming, David J and Hitzig, Zoe and Ong, Christopher and Shan, Carl Yan and Wadman, Kevin},
  abstract = {Despite the rapid adoption of LLM chatbots, little is known about how they are used. We document the growth of ChatGPT's consumer product from its launch in November 2022 through July 2025, when it had been adopted by around 10\% of the world's adult population. Early adopters were disproportionately male but the gender gap has narrowed dramatically, and we find higher growth rates in lower-income countries. Using a privacy-preserving automated pipeline, we classify usage patterns within a representative sample of ChatGPT conversations. We find steady growth in work-related messages but even faster growth in non-work-related messages, which have grown from 53\% to more than 70\% of all usage. Work usage is more common for educated users in highly-paid professional occupations. We classify messages by conversation topic and find that ``Practical Guidance,'' ``Seeking Information,'' and ``Writing'' are the three most common topics and collectively account for nearly 80\% of all conversations. Writing dominates work-related tasks, highlighting chatbots' unique ability to generate digital outputs compared to traditional search engines. Computer programming and self-expression both represent relatively small shares of use. Overall, we find that ChatGPT provides economic value through decision support, which is especially important in knowledge-intensive jobs.},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/MUN9WPVW/Chatterji et al. - How People Use ChatGPT.pdf}
}

@misc{cobbeTrainingVerifiersSolve2021,
  title = {Training {{Verifiers}} to {{Solve Math Word Problems}}},
  author = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  year = 2021,
  month = nov,
  number = {arXiv:2110.14168},
  eprint = {2110.14168},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2110.14168},
  urldate = {2025-09-29},
  abstract = {State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/yiyuanli/Zotero/storage/2HJNYJIJ/Cobbe et al. - 2021 - Training Verifiers to Solve Math Word Problems.pdf}
}

@article{collinsBuildingMachinesThat2024,
  title = {Building Machines That Learn and Think with People},
  author = {Collins, Katherine M. and Sucholutsky, Ilia and Bhatt, Umang and Chandra, Kartik and Wong, Lionel and Lee, Mina and Zhang, Cedegao E. and {Zhi-Xuan}, Tan and Ho, Mark and Mansinghka, Vikash and Weller, Adrian and Tenenbaum, Joshua B. and Griffiths, Thomas L.},
  year = 2024,
  month = oct,
  journal = {Nature Human Behaviour},
  volume = {8},
  number = {10},
  pages = {1851--1863},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-024-01991-9},
  urldate = {2025-12-30},
  abstract = {What do we want from machine intelligence? We envision machines that are not just tools for thought but partners in thought: reasonable, insightful, knowledgeable, reliable and trustworthy systems that think with us. Current artificial intelligence systems satisfy some of these criteria, some of the time. In this Perspective, we show how the science of collaborative cognition can be put to work to engineer systems that really can be called `thought partners', systems built to meet our expectations and complement our limitations. We lay out several modes of collaborative thought in which humans and artificial intelligence thought partners can engage, and we propose desiderata for human-compatible thought partnerships. Drawing on motifs from computational cognitive science, we motivate an alternative scaling path for the design of thought partners and ecosystems around their use through a Bayesian lens, whereby the partners we construct actively build and reason over models of the human and world.},
  copyright = {2024 Springer Nature Limited},
  langid = {english},
  keywords = {Computer science,Human behaviour}
}

@incollection{csiszarInformationTheoryCoding1982,
  title = {Information {{Theory}}: {{Coding Theorems}} for {{Discrete Memoryless Systems}}},
  shorttitle = {Information {{Theory}}},
  booktitle = {Information {{Theory}}},
  author = {Csisz{\'a}r, Imre and K{\"o}rner, J{\'a}nos and Birnbaun, Z. W. and Lukacs, E.},
  year = 1982,
  publisher = {Elsevier Science \& Technology},
  address = {United States},
  isbn = {978-0-12-198450-2},
  langid = {english},
  keywords = {Applied mathematics}
}

@misc{DefeatingNondeterminismLLM2025,
  title = {Defeating {{Nondeterminism}} in {{LLM Inference}}},
  year = 2025,
  month = sep,
  journal = {Thinking Machines Lab},
  urldate = {2025-09-11},
  abstract = {Reproducibility is a bedrock of scientific progress. However, it's remarkably difficult to get reproducible results out of large language models. For example, you might observe that asking ChatGPT the same question multiple times provides different results. This by itself is not surprising, since getting a result from a language model involves ``sampling'', a process that converts the language model's output into a probability distribution and probabilistically selects a token. What might be more surprising is that even when we adjust the temperature down to 0This means that the LLM always chooses the highest probability token, which is called greedy sampling. (thus making the sampling theoretically deterministic), LLM APIs are still not deterministic in practice (see past discussions here, here, or here). Even when running inference on your own hardware with an OSS inference library like vLLM or SGLang, sampling still isn't deterministic (see here or here).},
  chapter = {blog},
  howpublished = {https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/YS5JE9YX/defeating-nondeterminism-in-llm-inference.html}
}

@misc{deitkeMolmoPixMoOpen2024,
  title = {Molmo and {{PixMo}}: {{Open Weights}} and {{Open Data}} for {{State-of-the-Art Vision-Language Models}}},
  shorttitle = {Molmo and {{PixMo}}},
  author = {Deitke, Matt and Clark, Christopher and Lee, Sangho and Tripathi, Rohun and Yang, Yue and Park, Jae Sung and Salehi, Mohammadreza and Muennighoff, Niklas and Lo, Kyle and Soldaini, Luca and Lu, Jiasen and Anderson, Taira and Bransom, Erin and Ehsani, Kiana and Ngo, Huong and Chen, YenSung and Patel, Ajay and Yatskar, Mark and {Callison-Burch}, Chris and Head, Andrew and Hendrix, Rose and Bastani, Favyen and VanderBilt, Eli and Lambert, Nathan and Chou, Yvonne and Chheda, Arnavi and Sparks, Jenna and Skjonsberg, Sam and Schmitz, Michael and Sarnat, Aaron and Bischoff, Byron and Walsh, Pete and Newell, Chris and Wolters, Piper and Gupta, Tanmay and Zeng, Kuo-Hao and Borchardt, Jon and Groeneveld, Dirk and Nam, Crystal and Lebrecht, Sophie and Wittlif, Caitlin and Schoenick, Carissa and Michel, Oscar and Krishna, Ranjay and Weihs, Luca and Smith, Noah A. and Hajishirzi, Hannaneh and Girshick, Ross and Farhadi, Ali and Kembhavi, Aniruddha},
  year = 2024,
  month = dec,
  number = {arXiv:2409.17146},
  eprint = {2409.17146},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.17146},
  urldate = {2025-09-27},
  abstract = {Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q\&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a welltuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/yiyuanli/Zotero/storage/RCRUB8EQ/Deitke et al. - 2024 - Molmo and PixMo Open Weights and Open Data for State-of-the-Art Vision-Language Models.pdf}
}

@inproceedings{dengTextTupleTableInformationIntegration2024,
  title = {Text-{{Tuple-Table}}: {{Towards Information Integration}} in {{Text-to-Table Generation}} via {{Global Tuple Extraction}}},
  shorttitle = {Text-{{Tuple-Table}}},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Deng, Zheye and Chan, Chunkit and Wang, Weiqi and Sun, Yuxi and Fan, Wei and Zheng, Tianshi and Yim, Yauwai and Song, Yangqiu},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = 2024,
  month = nov,
  pages = {9300--9322},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.523},
  urldate = {2024-12-16},
  abstract = {The task of condensing large chunks of textual information into concise and structured tables has gained attention recently due to the emergence of Large Language Models (LLMs) and their potential benefit for downstream tasks, such as text summarization and text mining. Previous approaches often generate tables that directly replicate information from the text, limiting their applicability in broader contexts, as text-to-table generation in real-life scenarios necessitates information extraction, reasoning, and integration. However, there is a lack of both datasets and methodologies towards this task. In this paper, we introduce LiveSum, a new benchmark dataset created for generating summary tables of competitions based on real-time commentary texts. We evaluate the performances of state-of-the-art LLMs on this task in both fine-tuning and zero-shot settings, and additionally propose a novel pipeline called T\textasciicircum 3(Text-Tuple-Table) to improve their performances. Extensive experimental results demonstrate that LLMs still struggle with this task even after fine-tuning, while our approach can offer substantial performance gains without explicit training. Further analyses demonstrate that our method exhibits strong generalization abilities, surpassing previous approaches on several other text-to-table datasets. Our codeand data can be found at https://github.com/HKUST-KnowComp/LiveSum.},
  file = {/Users/yiyuanli/Zotero/storage/2NFRBSNP/Deng et al. - 2024 - Text-Tuple-Table Towards Information Integration .pdf}
}

@article{dincerPoliticalCultureCorruption2017,
  title = {Political {{Culture}} and {{Corruption Issues}} in {{State Politics}}: {{A New Measure}} of {{Corruption Issues}} and a {{Test}} of {{Relationships}} to {{Political Culture}}},
  shorttitle = {Political {{Culture}} and {{Corruption Issues}} in {{State Politics}}},
  author = {Dincer, Oguzhan and Johnston, Michael},
  year = 2017,
  journal = {Publius},
  volume = {47},
  number = {1},
  eprint = {44505500},
  eprinttype = {jstor},
  pages = {131--148},
  publisher = {[CSF Associates Inc., Oxford University Press]},
  issn = {0048-5950},
  urldate = {2024-01-25},
  abstract = {To what extent does variation in political culture influence the number and types of corruption issues arising in U.S. states? Drawing upon Daniel Elazar's enduring typology of American political subcultures and using a new news-based measure of corruption issues, we find that political culture remains a "sticky" deep determinant of the distribution of corruption issues across states. Unlike previous empirical studies we do not treat political culture as a set of consensus values, but rather emphasize tensions that can arise when multiple political cultures coexist in a state. Our findings are of interest not only with respect to the enduring role of culture itself, but also in terms of how it might help us understand the growing number of actions that are legal, yet still seen by many as corrupt.},
  file = {/Users/yiyuanli/Zotero/storage/887XKTG8/Dincer and Johnston - 2017 - Political Culture and Corruption Issues in State P.pdf}
}

@misc{dongReinforcementPreTraining2025,
  title = {Reinforcement {{Pre-Training}}},
  author = {Dong, Qingxiu and Dong, Li and Tang, Yao and Ye, Tianzhu and Sun, Yutao and Sui, Zhifang and Wei, Furu},
  year = 2025,
  month = jun,
  number = {arXiv:2506.08007},
  eprint = {2506.08007},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.08007},
  urldate = {2025-08-30},
  abstract = {In this work, we introduce Reinforcement Pre-Training (RPT) as a new scaling paradigm for large language models and reinforcement learning (RL). Specifically, we reframe next-token prediction as a reasoning task trained using RL, where it receives verifiable rewards for correctly predicting the next token for a given context. RPT offers a scalable method to leverage vast amounts of text data for general-purpose RL, rather than relying on domain-specific annotated answers. By incentivizing the capability of next-token reasoning, RPT significantly improves the language modeling accuracy of predicting the next tokens. Moreover, RPT provides a strong pre-trained foundation for further reinforcement fine-tuning. The scaling curves show that increased training compute consistently improves the next-token prediction accuracy. The results position RPT as an effective and promising scaling paradigm to advance language model pre-training.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/yiyuanli/Zotero/storage/XMKL6ZQ5/Dong et al. - 2025 - Reinforcement Pre-Training.pdf}
}

@misc{DownloadDataGapminder,
  title = {Download the Data \textbar{} {{Gapminder}}},
  urldate = {2025-11-24},
  langid = {american},
  file = {/Users/yiyuanli/Zotero/storage/YYPG2Z2G/data.html}
}

@article{edwardsGenderImperativesHonglou1990,
  title = {Gender {{Imperatives}} in {{Honglou}} Meng: {{Baoyu}}'s {{Bisexuality}}},
  shorttitle = {Gender {{Imperatives}} in {{Honglou}} Meng},
  author = {Edwards, Louise},
  year = 1990,
  journal = {Chinese Literature: Essays, Articles, Reviews (CLEAR)},
  volume = {12},
  eprint = {495224},
  eprinttype = {jstor},
  pages = {69--81},
  publisher = {Chinese Literature: essays, articles, reviews (CLEAR)},
  issn = {0161-9705},
  doi = {10.2307/495224},
  urldate = {2023-10-23},
  file = {/Users/yiyuanli/Zotero/storage/T48B59V8/Edwards - 1990 - Gender Imperatives in Honglou meng Baoyu's Bisexu.pdf}
}

@misc{EmpiricalAnalysisLarge,
  title = {An {{Empirical Analysis}} on {{Large Language Models}} in {{Debate Evaluation}}},
  urldate = {2025-09-22},
  howpublished = {https://arxiv.org/html/2406.00050v1},
  file = {/Users/yiyuanli/Zotero/storage/PHPPVI8T/2406.html}
}

@book{EpistemologyAnthology2nd,
  title = {Epistemology {{An Anthology}}, 2nd Edition ({{Blackwell Philosophy Anthologies}}) by {{Ernest Sosa}}, {{Jaegwon Kim}}, {{Jeremy Fantl}}, {{Matthew McGrath}} (z-Lib.Org).Pdf},
  file = {/Users/yiyuanli/Zotero/storage/ZZBRXFY6/Epistemology An Anthology, 2nd edition (Blackwell Philosophy Anthologies) by Ernest Sosa, Jaegwon Kim, Jeremy Fantl, Matthew McGrath (z-lib.org).pdf}
}

@misc{fakhfourVideoAlignmentUsing2024,
  title = {Video Alignment Using Unsupervised Learning of Local and Global Features},
  author = {Fakhfour, Niloufar and ShahverdiKondori, Mohammad and Hashembeiki, Sajjad and Norouzi, Mohammadjavad and Mohammadzade, Hoda},
  year = 2024,
  month = sep,
  number = {arXiv:2304.06841},
  eprint = {2304.06841},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.06841},
  urldate = {2025-09-26},
  abstract = {In this paper, we tackle the problem of video alignment, the process of matching the frames of a pair of videos containing similar actions. The main challenge in video alignment is that accurate correspondence should be established despite the differences in the execution processes and appearances between the two videos. We introduce an unsupervised method for alignment that uses global and local features of the frames. In particular, we introduce effective features for each video frame by means of three machine vision tools: person detection, pose estimation, and VGG network. Then the features are processed and combined to construct a multidimensional time series that represent the video. The resulting time series are used to align videos of the same actions using a novel version of dynamic time warping named Diagonalized Dynamic Time Warping(DDTW). The main advantage of our approach is that no training is required, which makes it applicable for any new type of action without any need to collect training samples for it. Additionally, our approach can be used for framewise labeling of action phases in a dataset with only a few labeled videos. For evaluation, we considered video synchronization and phase classification tasks on the Penn action [39] and subset of UCF101 [34] datasets. Also, for an effective evaluation of the video synchronization task, we present a new metric called Enclosed Area Error(EAE). The results show that our method outperforms previous state-of-the-art methods, such as TCC [11], and other selfsupervised and weakly supervised methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/yiyuanli/Zotero/storage/BAXZ2X7Y/Fakhfour et al. - 2024 - Video alignment using unsupervised learning of local and global features.pdf}
}

@misc{fanStableSegmentAnything2023,
  title = {Stable {{Segment Anything Model}}},
  author = {Fan, Qi and Tao, Xin and Ke, Lei and Ye, Mingqiao and Zhang, Yuan and Wan, Pengfei and Wang, Zhongyuan and Tai, Yu-Wing and Tang, Chi-Keung},
  year = 2023,
  month = dec,
  number = {arXiv:2311.15776},
  eprint = {2311.15776},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.15776},
  urldate = {2025-09-28},
  abstract = {The Segment Anything Model (SAM) achieves remarkable promptable segmentation given high-quality prompts which, however, often require good skills to specify. To make SAM robust to casual prompts, this paper presents the first comprehensive analysis on SAM's segmentation stability across a diverse spectrum of prompt qualities, notably imprecise bounding boxes and insufficient points. Our key finding reveals that given such low-quality prompts, SAM's mask decoder tends to activate image features that are biased towards the background or confined to specific object parts. To mitigate this issue, our key idea consists of calibrating solely SAM's mask attention by adjusting the sampling locations and amplitudes of image features, while the original SAM model architecture and weights remain unchanged. Consequently, our deformable sampling plugin (DSP) enables SAM to adaptively shift attention to the prompted target regions in a data-driven manner. During inference, dynamic routing plugin (DRP) is proposed that toggles SAM between the deformable and regular grid sampling modes, conditioned on the input prompt quality. Thus, our solution, termed Stable-SAM, offers several advantages: 1) improved SAM's segmentation stability across a wide range of prompt qualities, while 2) retaining SAM's powerful promptable segmentation efficiency and generality, with 3) minimal learnable parameters (0.08 M) and fast adaptation. Extensive experiments validate the effectiveness and advantages of our approach, underscoring StableSAM as a more robust solution for segmenting anything.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/yiyuanli/Zotero/storage/SZEKGH3C/Fan et al. - 2023 - Stable Segment Anything Model.pdf}
}

@misc{franceschelliTrainingFoundationModels2025,
  title = {Training {{Foundation Models}} as {{Data Compression}}: {{On Information}}, {{Model Weights}} and {{Copyright Law}}},
  shorttitle = {Training {{Foundation Models}} as {{Data Compression}}},
  author = {Franceschelli, Giorgio and Cevenini, Claudia and Musolesi, Mirco},
  year = 2025,
  month = mar,
  number = {arXiv:2407.13493},
  eprint = {2407.13493},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.13493},
  urldate = {2025-12-18},
  abstract = {The training process of foundation models as for other classes of deep learning systems is based on minimizing the reconstruction error over a training set. For this reason, they are susceptible to the memorization and subsequent reproduction of training samples. In this paper, we introduce a training-as-compressing perspective, wherein the model's weights embody a compressed representation of the training data. From a copyright standpoint, this point of view implies that the weights can be considered a reproduction or, more likely, a derivative work of a potentially protected set of works. We investigate the technical and legal challenges that emerge from this framing of the copyright of outputs generated by foundation models, including their implications for practitioners and researchers. We demonstrate that adopting an information-centric approach to the problem presents a promising pathway for tackling these emerging complex legal issues.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/yiyuanli/Zotero/storage/LV6Y89GN/Franceschelli et al. - 2025 - Training Foundation Models as Data Compression On Information, Model Weights and Copyright Law.pdf;/Users/yiyuanli/Zotero/storage/44I2DI96/2407.html}
}

@misc{Gaia2AREEmpowering,
  title = {Gaia2 and {{ARE}}: {{Empowering}} the Community to Study Agents},
  shorttitle = {Gaia2 and {{ARE}}},
  urldate = {2025-09-23},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  file = {/Users/yiyuanli/Zotero/storage/HT947TIH/gaia2.html}
}

@article{geddesHowCasesYou1990,
  title = {How the {{Cases You Choose Affect}} the {{Answers You Get}}: {{Selection Bias}} in {{Comparative Politics}}},
  shorttitle = {How the {{Cases You Choose Affect}} the {{Answers You Get}}},
  author = {Geddes, Barbara},
  year = 1990,
  journal = {Political Analysis},
  volume = {2},
  pages = {131--150},
  issn = {1047-1987, 1476-4989},
  doi = {10.1093/pan/2.1.131},
  urldate = {2021-09-11},
  abstract = {This article demonstrates how the selection of cases for study on the basis of outcomes on the dependent variable biases conclusions. It first lays out the logic of explanation and shows how it is violated when only cases that have achieved the outcome of interest are studied. It then examines three well known and highly regarded studies in the field of comparative politics, com paring the conclusions reached in the original work with a test of the argu ments on cases selected without regard for their position on the dependent variable. In each instance, conclusions based on the uncorrelated sample differ from the original conclusions.},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/7RUURZQW/How the Cases You Choose .pdf}
}

@book{grolemundDataScience2016,
  title = {R for Data Science},
  author = {Grolemund, Garrett},
  year = 2016,
  journal = {R for data science},
  publisher = {Hadley Wickham},
  address = {Place of publication not identified},
  abstract = {"An online version of this book...will continue to evolve in between reprints of the physical book. The source of the book is available at...which makes it easy to turn R markdown files into HTML, PDF, and EPUB"--Colophon from website version of book.},
  collaborator = {Wickham, Hadley},
  isbn = {9781491910399},
  langid = {english},
  keywords = {Computer programs,Data mining,Data processing,Information visualization,Mathematical statistics,R (Computer program language)}
}

@article{guarinoOntologiesKnowledgeBases,
  title = {Ontologies and {{Knowledge Bases}}},
  author = {Guarino, Nicola and Giaretta, Pierdaniele},
  abstract = {The word ``ontology'' has recently gained a good popularity within the knowledge engineering community. However, its meaning tends to remain a bit vague, as the term is used in very different ways. Limiting our attention to the various proposals made in the current debate in AI, we isolate a number of interpretations, which in our opinion deserve a suitable clarification. We elucidate the implications of such various interpretations, arguing for the need of clear terminological choices regarding the technical use of terms like ``ontology'', ``conceptualization'' and ``ontological commitment''. After some comments on the use ``Ontology'' (with the capital ``o'') as a term which denotes a philosophical discipline, we analyse the possible confusion between an ontology intended as a particular conceptual framework at the knowledge level and an ontology intended as a concrete artifact at the symbol level, to be used for a given purpose. A crucial point in this clarification effort is the careful analysis of Gruber' s definition of an ontology as a specification of a conceptualization.},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/2DIU3ZQL/Guarino and Giaretta - Ontologies and Knowledge Bases.pdf}
}

@article{guarinoOntologiesKnowledgeBases1995,
  title = {Ontologies and {{Knowledge Bases}}},
  author = {Guarino, Nicola and Giaretta, Pierdaniele},
  year = 1995,
  abstract = {The word ``ontology'' has recently gained a good popularity within the knowledge engineering community. However, its meaning tends to remain a bit vague, as the term is used in very different ways. Limiting our attention to the various proposals made in the current debate in AI, we isolate a number of interpretations, which in our opinion deserve a suitable clarification. We elucidate the implications of such various interpretations, arguing for the need of clear terminological choices regarding the technical use of terms like ``ontology'', ``conceptualization'' and ``ontological commitment''. After some comments on the use ``Ontology'' (with the capital ``o'') as a term which denotes a philosophical discipline, we analyse the possible confusion between an ontology intended as a particular conceptual framework at the knowledge level and an ontology intended as a concrete artifact at the symbol level, to be used for a given purpose. A crucial point in this clarification effort is the careful analysis of Gruber' s definition of an ontology as a specification of a conceptualization.},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/7V2MJHBR/Guarino and Giaretta - Ontologies and Knowledge Bases.pdf}
}

@inproceedings{guoSampleDesignEngineering2024,
  title = {Sample {{Design Engineering}}: {{An Empirical Study}} on {{Designing Better Fine-Tuning Samples}} for {{Information Extraction}} with {{LLMs}}},
  shorttitle = {Sample {{Design Engineering}}},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{Industry Track}}},
  author = {Guo, Biyang and Wang, He and Xiao, Wenyilin and Chen, Hong and Lee, ZhuXin and Han, Songqiao and Huang, Hailiang},
  editor = {Dernoncourt, Franck and {Preo{\c t}iuc-Pietro}, Daniel and Shimorina, Anastasia},
  year = 2024,
  month = nov,
  pages = {573--594},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, US},
  doi = {10.18653/v1/2024.emnlp-industry.43},
  urldate = {2024-12-16},
  abstract = {Large language models (LLMs) have achieved significant leadership in many NLP tasks, but aligning structured output with generative models in information extraction (IE) tasks remains a challenge. Prompt Engineering (PE) is renowned for improving IE performance through prompt modifications. However, the realm of the sample design for downstream fine-tuning, crucial for task-specific LLM adaptation, is largely unexplored. This paper introduces **Sample Design Engineering** (SDE), a methodical approach to enhancing LLMs' post-tuning performance on IE tasks by refining input, output, and reasoning designs. Through extensive ID and OOD experiments across six LLMs, we first assess the impact of various design options on IE performance, revealing several intriguing patterns. Based on these insights, we then propose an integrated SDE strategy and validate its consistent superiority over heuristic sample designs on three complex IE tasks with four additional LLMs, demonstrating the generality of our method. Additionally, analyses of LLMs' inherent prompt/output perplexity, zero-shot, and ICL abilities illustrate that good PE strategies may not always translate to good SDE strategies.},
  file = {/Users/yiyuanli/Zotero/storage/V5XT79CR/Guo et al. - 2024 - Sample Design Engineering An Empirical Study on D.pdf}
}

@article{guRadarBenchmarkingLanguage,
  title = {Radar: {{Benchmarking Language Models}} on {{Imperfect Tabular Data}}},
  author = {Gu, Ken and Zhang, Zhihan and Lin, Kate and Zhang, Yuwei and Paruchuri, Akshay and Yu, Hong and Kazemi, Mehran and Ayush, Kumar and Heydari, A Ali and Xu, Maxwell A and Narayanswamy, Girish and Liu, Yun and Poh, Ming-Zher and Malhotra, Mark and Patel, Shwetak and Palangi, Hamid and Xu, Xuhai and McDuff, Daniel and Althoff, Tim and Liu, Xin},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/JJJ8Q5QI/Gu et al. - Radar Benchmarking Language Models on Imperfect Tabular Data.pdf}
}

@article{hammonsWasJamesMadison1999,
  title = {Was {{James Madison Wrong}}? {{Rethinking}} the {{American Preference}} for {{Short}}, {{Framework-Oriented Constitutions}}},
  shorttitle = {Was {{James Madison Wrong}}?},
  author = {Hammons, Christopher W.},
  year = 1999,
  journal = {The American Political Science Review},
  volume = {93},
  number = {4},
  eprint = {2586116},
  eprinttype = {jstor},
  pages = {837--849},
  publisher = {[American Political Science Association, Cambridge University Press]},
  issn = {0003-0554},
  doi = {10.2307/2586116},
  urldate = {2024-01-28},
  abstract = {American constitutional thought has long held that short, framework-oriented constitutions last longer than lengthy, statute-oriented constitutions. The longevity of the U.S. Constitution contributes heavily to this assumption. Not surprisingly, political scientists criticize state constitutions for their greater length and tendency to address issues better dealt with through ordinary statute law. These "defects" are frequently cited as responsible for the shorter lifespan of state constitutions. An examination of the 145 constitutions used by the American states since 1776, however, reveals a relationship among content, length, and durability that refutes the assumption that the design of the national constitution is necessarily superior. To the contrary, the analysis here reveals that longer and more detailed design of state constitutions actually enhances rather than reduces their longevity.},
  file = {/Users/yiyuanli/Zotero/storage/K9VPKIQ3/Hammons - 1999 - Was James Madison Wrong Rethinking the American P.pdf}
}

@misc{hanPreTrainedModelsPresent2021,
  title = {Pre-{{Trained Models}}: {{Past}}, {{Present}} and {{Future}}},
  shorttitle = {Pre-{{Trained Models}}},
  author = {Han, Xu and Zhang, Zhengyan and Ding, Ning and Gu, Yuxian and Liu, Xiao and Huo, Yuqi and Qiu, Jiezhong and Yao, Yuan and Zhang, Ao and Zhang, Liang and Han, Wentao and Huang, Minlie and Jin, Qin and Lan, Yanyan and Liu, Yang and Liu, Zhiyuan and Lu, Zhiwu and Qiu, Xipeng and Song, Ruihua and Tang, Jie and Wen, Ji-Rong and Yuan, Jinhui and Zhao, Wayne Xin and Zhu, Jun},
  year = 2021,
  month = aug,
  number = {arXiv:2106.07139},
  eprint = {2106.07139},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.07139},
  urldate = {2025-12-18},
  abstract = {Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success and become a milestone in the field of artificial intelligence (AI). Owing to sophisticated pre-training objectives and huge model parameters, large-scale PTMs can effectively capture knowledge from massive labeled and unlabeled data. By storing knowledge into huge parameters and fine-tuning on specific tasks, the rich knowledge implicitly encoded in huge parameters can benefit a variety of downstream tasks, which has been extensively demonstrated via experimental verification and empirical analysis. It is now the consensus of the AI community to adopt PTMs as backbone for downstream tasks rather than learning models from scratch. In this paper, we take a deep look into the history of pre-training, especially its special relation with transfer learning and self-supervised learning, to reveal the crucial position of PTMs in the AI development spectrum. Further, we comprehensively review the latest breakthroughs of PTMs. These breakthroughs are driven by the surge of computational power and the increasing availability of data, towards four important directions: designing effective architectures, utilizing rich contexts, improving computational efficiency, and conducting interpretation and theoretical analysis. Finally, we discuss a series of open problems and research directions of PTMs, and hope our view can inspire and advance the future study of PTMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/yiyuanli/Zotero/storage/RQW4VCVE/Han et al. - 2021 - Pre-Trained Models Past, Present and Future.pdf;/Users/yiyuanli/Zotero/storage/TIY63N85/2106.html}
}

@misc{harakSelfSupervisedLearningContext2024,
  title = {Self-{{Supervised Learning}} in the {{Context}} of {{LLMs}}},
  author = {Harak, Saurabh},
  year = 2024,
  month = oct,
  journal = {Medium},
  urldate = {2025-12-18},
  abstract = {Self-Supervised Learning: Concepts and Implementation},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/7PDHZ4FT/self-supervised-learning-in-the-context-of-llms-5ae7fb729a38.html}
}

@misc{he2025nondeterminism,
  title = {Defeating Nondeterminism in {{LLM}} Inference},
  author = {He, Horace and Lab, Thinking Machines},
  year = 2025,
  journal = {Thinking Machines Lab: Connectionism}
}

@misc{hendrycksMeasuringMassiveMultitask2021,
  title = {Measuring {{Massive Multitask Language Understanding}}},
  author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  year = 2021,
  month = jan,
  number = {arXiv:2009.03300},
  eprint = {2009.03300},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2009.03300},
  urldate = {2025-09-29},
  abstract = {We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/yiyuanli/Zotero/storage/YDVVXN39/Hendrycks et al. - 2021 - Measuring Massive Multitask Language Understanding.pdf;/Users/yiyuanli/Zotero/storage/PGJAD55L/2009.html}
}

@misc{hendrycksMeasuringMassiveMultitask2021a,
  title = {Measuring {{Massive Multitask Language Understanding}}},
  author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  year = 2021,
  month = jan,
  number = {arXiv:2009.03300},
  eprint = {2009.03300},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2009.03300},
  urldate = {2025-09-29},
  abstract = {We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/yiyuanli/Zotero/storage/KBP6TV2N/Hendrycks et al. - 2021 - Measuring Massive Multitask Language Understanding.pdf;/Users/yiyuanli/Zotero/storage/6RUZAJN8/2009.html}
}

@misc{hendrycksMeasuringMathematicalProblem2021,
  title = {Measuring {{Mathematical Problem Solving With}} the {{MATH Dataset}}},
  author = {Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  year = 2021,
  month = nov,
  number = {arXiv:2103.03874},
  eprint = {2103.03874},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.03874},
  urldate = {2025-09-29},
  abstract = {Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/yiyuanli/Zotero/storage/ERW57CKJ/Hendrycks et al. - 2021 - Measuring Mathematical Problem Solving With the MATH Dataset.pdf}
}

@misc{horeceDefeatingNondeterminismLLM,
  title = {Defeating {{Nondeterminism}} in {{LLM Inference}}},
  author = {Horece, He},
  journal = {Thinking Machines Lab},
  urldate = {2025-10-06},
  abstract = {Reproducibility is a bedrock of scientific progress. However, it's remarkably difficult to get reproducible results out of large language models. For example, you might observe that asking ChatGPT the same question multiple times provides different results. This by itself is not surprising, since getting a result from a language model involves ``sampling'', a process that converts the language model's output into a probability distribution and probabilistically selects a token. What might be more surprising is that even when we adjust the temperature down to 0This means that the LLM always chooses the highest probability token, which is called greedy sampling. (thus making the sampling theoretically deterministic), LLM APIs are still not deterministic in practice (see past discussions here, here, or here). Even when running inference on your own hardware with an OSS inference library like vLLM or SGLang, sampling still isn't deterministic (see here or here).},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/EN7FTYCZ/defeating-nondeterminism-in-llm-inference.html}
}

@misc{huberIntroductionCausalDiscovery2024,
  title = {An {{Introduction}} to {{Causal Discovery}}},
  author = {Huber, Martin},
  year = 2024,
  month = jul,
  number = {arXiv:2407.08602},
  eprint = {2407.08602},
  primaryclass = {econ},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.08602},
  urldate = {2024-09-28},
  abstract = {In social sciences and economics, causal inference traditionally focuses on assessing the impact of predefined treatments (or interventions) on predefined outcomes, such as the effect of education programs on earnings. Causal discovery, in contrast, aims to uncover causal relationships among multiple variables in a data-driven manner, by investigating statistical associations rather than relying on predefined causal structures. This approach, more common in computer science, seeks to understand causality in an entire system of variables, which can be visualized by causal graphs. This survey provides an introduction to key concepts, algorithms, and applications of causal discovery from the perspectives of economics and social sciences. It covers fundamental concepts like d-separation, causal faithfulness, and Markov equivalence, sketches various algorithms for causal discovery, and discusses the back-door and front-door criteria for identifying causal effects. The survey concludes with more specific examples of causal discovery, e.g. for learning all variables that directly affect an outcome of interest and/or testing identification of causal effects in observational data.},
  archiveprefix = {arXiv},
  keywords = {Economics - Econometrics},
  file = {/Users/yiyuanli/Zotero/storage/EQXZ7UUA/Huber - 2024 - An Introduction to Causal Discovery.pdf;/Users/yiyuanli/Zotero/storage/2URCUHR9/2407.html}
}

@misc{IntroductionLinearAlgebra,
  title = {Introduction to Linear Algebra - {{Reed College}}},
  urldate = {2023-11-15},
  howpublished = {https://catalog.library.reed.edu/discovery/fulldisplay?docid=alma99900023605401841\&context=L\&vid=01ALLIANCE\_REED:REED\&lang=en\&search\_scope=all\&adaptor=Local\%20Search\%20Engine\&tab=all\&query=any,contains,Introduction\%20to\%20Linear\%20Algebra\%20Strang},
  file = {/Users/yiyuanli/Zotero/storage/KMX2BLJ3/fulldisplay.html}
}

@misc{IPEDSAnalyticsDelta,
  title = {{{IPEDS Analytics}}: {{Delta Cost Project Database}}},
  urldate = {2023-12-06},
  howpublished = {https://nces.ed.gov/ipeds/use-the-data/delta-cost-project-finance-data}
}

@misc{ipHowBuildLLM2025,
  title = {How to {{Build}} an {{LLM Evaluation Framework}}, from {{Scratch}} - {{Confident AI}}},
  author = {Ip, Jeffrey},
  year = 2025,
  month = aug,
  urldate = {2025-08-30},
  abstract = {In this article, you're going to learn how to build the world's most robust and scalable LLM evaluation framework.},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/A5Z2TMEV/how-to-build-an-llm-evaluation-framework-from-scratch.html}
}

@misc{ipLLMEvaluationMetrics2025,
  title = {{{LLM Evaluation Metrics}}: {{The Ultimate LLM Evaluation Guide}} - {{Confident AI}}},
  shorttitle = {{{LLM Evaluation Metrics}}},
  author = {Ip, Jeffrey},
  year = 2025,
  month = sep,
  urldate = {2025-09-22},
  abstract = {In this article, I'll walkthrough everything you need to know about LLM evaluation metrics, with code samples.},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/W3IP8GTF/llm-evaluation-metrics-everything-you-need-for-llm-evaluation.html}
}

@article{jimenezSWEBENCHCANLANGUAGE2024,
  title = {{{SWE-BENCH}}: {{CAN LANGUAGE MODELS RESOLVE REAL-WORLD GITHUB ISSUES}}?},
  author = {Jimenez, Carlos E and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik},
  year = 2024,
  abstract = {Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-ofthe-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere 1.96\% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/NLXFHKQM/Jimenez et al. - 2024 - SWE-BENCH CAN LANGUAGE MODELS RESOLVE REAL-WORLD GITHUB ISSUES.pdf}
}

@misc{jingDSBenchHowFar2025,
  title = {{{DSBench}}: {{How Far Are Data Science Agents}} from {{Becoming Data Science Experts}}?},
  shorttitle = {{{DSBench}}},
  author = {Jing, Liqiang and Huang, Zhehui and Wang, Xiaoyang and Yao, Wenlin and Yu, Wenhao and Ma, Kaixin and Zhang, Hongming and Du, Xinya and Yu, Dong},
  year = 2025,
  month = apr,
  number = {arXiv:2409.07703},
  eprint = {2409.07703},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.07703},
  urldate = {2025-12-15},
  abstract = {Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have demonstrated impressive language/vision reasoning abilities, igniting the recent trend of building agents for targeted applications such as shopping assistants or AI software engineers. Recently, many data science benchmarks have been proposed to investigate their performance in the data science domain. However, existing data science benchmarks still fall short when compared to real-world data science applications due to their simplified settings. To bridge this gap, we introduce DSBench, a comprehensive benchmark designed to evaluate data science agents with realistic tasks. This benchmark includes 466 data analysis tasks and 74 data modeling tasks, sourced from Eloquence and Kaggle competitions. DSBench offers a realistic setting by encompassing long contexts, multimodal task backgrounds, reasoning with large data files and multi-table structures, and performing end-to-end data modeling tasks. Our evaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle with most tasks, with the best agent solving only 34.12\% of data analysis tasks and achieving a 34.74\% Relative Performance Gap (RPG). These findings underscore the need for further advancements in developing more practical, intelligent, and autonomous data science agents.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/yiyuanli/Zotero/storage/4L53AN36/Jing et al. - 2025 - DSBench How Far Are Data Science Agents from Becoming Data Science Experts.pdf}
}

@misc{JuglabDivNoising2025,
  title = {Juglab/{{DivNoising}}},
  year = 2025,
  month = sep,
  urldate = {2025-10-08},
  abstract = {DivNoising is an unsupervised denoising method to generate diverse denoised samples for any noisy input image. This repository contains the code to reproduce the results reported in the paper https://openreview.net/pdf?id=agHLCOBM5jP},
  howpublished = {JugLab},
  keywords = {diverse-denoising,fully-convolutional-vae,noise-model,unsupervised-denoising}
}

@misc{kalaiWhyLanguageModels2025,
  title = {Why {{Language Models Hallucinate}}},
  author = {Kalai, Adam Tauman and Nachum, Ofir and Vempala, Santosh S. and Zhang, Edwin},
  year = 2025,
  month = sep,
  number = {arXiv:2509.04664},
  eprint = {2509.04664},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2509.04664},
  urldate = {2025-09-22},
  abstract = {Like students facing hard exam questions, large language models sometimes guess when uncertain, producing plausible yet incorrect statements instead of admitting uncertainty. Such "hallucinations" persist even in state-of-the-art systems and undermine trust. We argue that language models hallucinate because the training and evaluation procedures reward guessing over acknowledging uncertainty, and we analyze the statistical causes of hallucinations in the modern training pipeline. Hallucinations need not be mysterious -- they originate simply as errors in binary classification. If incorrect statements cannot be distinguished from facts, then hallucinations in pretrained language models will arise through natural statistical pressures. We then argue that hallucinations persist due to the way most evaluations are graded -- language models are optimized to be good test-takers, and guessing when uncertain improves test performance. This "epidemic" of penalizing uncertain responses can only be addressed through a socio-technical mitigation: modifying the scoring of existing benchmarks that are misaligned but dominate leaderboards, rather than introducing additional hallucination evaluations. This change may steer the field toward more trustworthy AI systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/yiyuanli/Zotero/storage/HW6X3A44/Kalai et al. - 2025 - Why Language Models Hallucinate.pdf;/Users/yiyuanli/Zotero/storage/3HK768ST/2509.html}
}

@misc{kaplanScalingLawsNeural2020,
  title = {Scaling {{Laws}} for {{Neural Language Models}}},
  author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  year = 2020,
  month = jan,
  number = {arXiv:2001.08361},
  eprint = {2001.08361},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2001.08361},
  urldate = {2025-12-18},
  abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/yiyuanli/Zotero/storage/QXQFLVYW/Kaplan et al. - 2020 - Scaling Laws for Neural Language Models.pdf;/Users/yiyuanli/Zotero/storage/X6UMHFZB/2001.html}
}

@article{kayStaggeredCellintrinsicTiming2005,
  title = {Staggered Cell-Intrinsic Timing of Ath5 Expression Underlies the Wave of Ganglion Cell Neurogenesis in the Zebrafish Retina},
  author = {Kay, Jeremy N. and Link, Brian A. and Baier, Herwig},
  year = 2005,
  month = jun,
  journal = {Development (Cambridge, England)},
  volume = {132},
  number = {11},
  pages = {2573--2585},
  issn = {0950-1991},
  doi = {10.1242/dev.01831},
  abstract = {In the developing nervous system, progenitor cells must decide when to withdraw from the cell cycle and commence differentiation. There is considerable debate whether cell-extrinsic or cell-intrinsic factors are most important for triggering this switch. In the vertebrate retina, initiation of neurogenesis has recently been explained by a 'sequential-induction' model--signals from newly differentiated neurons are thought to trigger neurogenesis in adjacent progenitors, creating a wave of neurogenesis that spreads across the retina in a stereotypical manner. We show here, however, that the wave of neurogenesis in the zebrafish retina can emerge through the independent action of progenitor cells--progenitors in different parts of the retina appear pre-specified to initiate neurogenesis at different times. We provide evidence that midline Sonic hedgehog signals, acting before the onset of neurogenesis, are part of the mechanism that sets the neurogenic timer in these cells. Our results highlight the importance of intrinsic factors for triggering neurogenesis, but they also suggest that early signals can modulate these intrinsic factors to influence the timing of neurogenesis many cell cycles later, thereby potentially coordinating axial patterning with control of neuron number and cell fate.},
  langid = {english},
  pmid = {15857917},
  keywords = {Animals,Cell Differentiation,Chimera,DNA Primers,DNA-Binding Proteins,Ganglia Sensory,Gene Expression Regulation Developmental,Growth Substances,Hedgehog Proteins,Lasers,Micromanipulation,Retinal Ganglion Cells,Reverse Transcriptase Polymerase Chain Reaction,Signal Transduction,Stem Cells,Time Factors,Trans-Activators,Veratrum Alkaloids,Zebrafish,Zebrafish Proteins}
}

@book{kingDesigningSocialInquiry1994,
  title = {Designing Social Inquiry: Scientific Inference in Qualitative Research},
  shorttitle = {Designing Social Inquiry},
  author = {King, Gary and Keohane, Robert O. and Verba, Sidney},
  year = 1994,
  publisher = {Princeton University Press},
  address = {Princeton, N.J},
  isbn = {978-0-691-03470-6 978-0-691-03471-3},
  langid = {english},
  lccn = {H61 .K5437 1994},
  keywords = {Inference,Methodology,Qualitative research,Research,Social sciences},
  file = {/Users/yiyuanli/Zotero/storage/5Z7AYQPZ/King 等。 - 1994 - Designing social inquiry scientific inference in .pdf}
}

@incollection{kornerCombinatorialProofNoisy1981,
  title = {About a {{Combinatorial Proof}} of the {{Noisy Channel Coding Theorem}}},
  booktitle = {Multi-{{User Communication Systems}}},
  author = {K{\"o}rner, J{\'a}nos},
  editor = {Longo, G.},
  year = 1981,
  series = {International {{Centre}} for {{Mechanical Sciences}}},
  pages = {49--72},
  publisher = {Springer},
  address = {Vienna},
  doi = {10.1007/978-3-7091-2900-5_3},
  urldate = {2023-10-26},
  abstract = {The most famous problem of information theory, that of determining the zero-error capacity of a discrete memory less channel, is of combinatorial nature. Originally stated by Shannon [1] in 1956, it has been studied by many combinatorialists. In a recent paper Lov\'asz [2] developed a sophisticated method to derive converse results on the zero-error capacity and succeeded to settle an intriguing special case. This is a channel of which the five input letters can be arranged cyclically so that two input letters can result in a same output letter with positive probability iff they are adjacent in this cyclical array. This ``pentagon'' constitutes the simplest case for which Shannon was unable to determine the zero-error capacity in 1956. Unfortunately, the Lov\'asz bounding technique also fails in many important cases, cf. Haemers [3]. It has often been argued that the problem is not intrinsically information-theoretic, since it can be stated without using probabilistic concepts. (This argument was even brought up as an excuse for the information theorists' inability to solve the problem.) In the last couple of years, however, an increasing number of people seem to believe that in the discrete case, all the classical results of information theory can be rederived using combinatorial methods. Moreover, the proofs so obtained often are simpler and more intuitive than earlier ones. The present tutorial paper should propagate this belief.},
  isbn = {978-3-7091-2900-5},
  langid = {english},
  keywords = {Block Code,Block Length,Conditional Entropy,Mutual Information,Stochastic Matrix}
}

@misc{krullNoise2VoidLearningDenoising2019,
  title = {{{Noise2Void}} - {{Learning Denoising}} from {{Single Noisy Images}}},
  author = {Krull, Alexander and Buchholz, Tim-Oliver and Jug, Florian},
  year = 2019,
  month = apr,
  number = {arXiv:1811.10980},
  eprint = {1811.10980},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1811.10980},
  urldate = {2025-10-08},
  abstract = {The field of image denoising is currently dominated by discriminative deep learning methods that are trained on pairs of noisy input and clean target images. Recently it has been shown that such methods can also be trained without clean targets. Instead, independent pairs of noisy images can be used, in an approach known as NOISE2NOISE (N2N). Here, we introduce NOISE2VOID (N2V), a training scheme that takes this idea one step further. It does not require noisy image pairs, nor clean target images. Consequently, N2V allows us to train directly on the body of data to be denoised and can therefore be applied when other methods cannot. Especially interesting is the application to biomedical image data, where the acquisition of training targets, clean or noisy, is frequently not possible. We compare the performance of N2V to approaches that have either clean target images and/or noisy image pairs available. Intuitively, N2V cannot be expected to outperform methods that have more information available during training. Still, we observe that the denoising performance of NOISE2VOID drops in moderation and compares favorably to training-free denoising methods.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/yiyuanli/Zotero/storage/MBFEQVJH/Krull et al. - 2019 - Noise2Void - Learning Denoising from Single Noisy Images.pdf}
}

@misc{laiDS1000NaturalReliable2022,
  title = {{{DS-1000}}: {{A Natural}} and {{Reliable Benchmark}} for {{Data Science Code Generation}}},
  shorttitle = {{{DS-1000}}},
  author = {Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Scott Wen-tau and Fried, Daniel and Wang, Sida and Yu, Tao},
  year = 2022,
  month = nov,
  number = {arXiv:2211.11501},
  eprint = {2211.11501},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2211.11501},
  urldate = {2025-12-15},
  abstract = {We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas. Compared to prior works, DS-1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases since we collected them from StackOverflow. Second, our automatic evaluation is highly specific (reliable) -- across all Codex-002-predicted solutions that our evaluation accept, only 1.8\% of them are incorrect; we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting API usages or keywords. Finally, we proactively defend against memorization by slightly modifying our problems to be different from the original StackOverflow source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training. The current best public system (Codex-002) achieves 43.3\% accuracy, leaving ample room for improvement. We release our benchmark at https://ds1000-code-gen.github.io.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Software Engineering},
  file = {/Users/yiyuanli/Zotero/storage/74DWP8WP/Lai et al. - 2022 - DS-1000 A Natural and Reliable Benchmark for Data Science Code Generation.pdf;/Users/yiyuanli/Zotero/storage/PSZNZ8TD/2211.html}
}

@misc{lambertTulu3Pushing2025,
  title = {Tulu 3: {{Pushing Frontiers}} in {{Open Language Model Post-Training}}},
  shorttitle = {Tulu 3},
  author = {Lambert, Nathan and Morrison, Jacob and Pyatkin, Valentina and Huang, Shengyi and Ivison, Hamish and Brahman, Faeze and Miranda, Lester James V. and Liu, Alisa and Dziri, Nouha and Lyu, Shane and Gu, Yuling and Malik, Saumya and Graf, Victoria and Hwang, Jena D. and Yang, Jiangjiang and Bras, Ronan Le and Tafjord, Oyvind and Wilhelm, Chris and Soldaini, Luca and Smith, Noah A. and Wang, Yizhong and Dasigi, Pradeep and Hajishirzi, Hannaneh},
  year = 2025,
  month = apr,
  number = {arXiv:2411.15124},
  eprint = {2411.15124},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.15124},
  urldate = {2025-09-27},
  abstract = {Language model post-training is applied to refine behaviors and unlock new skills across a wide range of language models, but open recipes for applying these techniques lag behind proprietary ones. The underlying training data and recipes for post-training are simultaneously the most important pieces of the puzzle and the portion with the least transparency. To bridge this gap, we introduce T\"ulu 3, a family of fully-open state-of-the-art post-trained models, alongside its data, code, and training recipes, serving as a comprehensive guide for modern post-training techniques. T\"ulu 3, which builds on Llama 3.1 base models, achieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude 3.5-Haiku. The training algorithms for our models include supervised finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we call Reinforcement Learning with Verifiable Rewards (RLVR). With T\"ulu 3, we build a multi-task evaluation scheme for post-training with development and unseen evaluations, standard benchmark implementations, and substantial decontamination of existing open datasets on said benchmarks. We conclude with analysis and discussion of training methods that did not reliably improve performance.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/yiyuanli/Zotero/storage/JS4RDT5D/Lambert et al. - 2025 - Tulu 3 Pushing Frontiers in Open Language Model Post-Training.pdf}
}

@misc{lanREINFORCEDrGRPO,
  title = {From {{REINFORCE}} to {{Dr}}. {{GRPO}}},
  author = {Lan, Qingfeng},
  urldate = {2025-09-01},
  abstract = {A Unified Perspective on LLM Post-training},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/IUYK5NZ9/llm_post_training.html}
}

@misc{LearningForgettingLLMs,
  title = {Learning Is {{Forgetting}}: {{LLMs}} Are a {{Lossy Compression}} of the {{Internet}} \textbar{} {{Natural}} and {{Artificial Minds}}},
  shorttitle = {Learning Is {{Forgetting}}},
  urldate = {2025-12-18},
  abstract = {Henry Conklin, Postdoctoral Research Fellow, Princeton Laboratory for Artificial Intelligence Despite the increasing prevalence of large language models (LLMs), we still have a limited understanding of how their representational spaces are structured. This limits our ability to interpret how and what they learn or relate them to learning in humans},
  howpublished = {https://nam.ai.princeton.edu/events/2025/learning-forgetting-llms-are-lossy-compression-internet},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/UY4TJRGJ/learning-forgetting-llms-are-lossy-compression-internet.html}
}

@article{leeSelfsupervisedDenoisingDynamic2025,
  title = {Self-Supervised Denoising of Dynamic Fluorescence Images via Temporal Gradient-Empowered Deep Learning},
  author = {Lee, Woojin and Jang, Minseok A. and Nam, Hyeong Soo and Song, Jeonggeun and Choi, Jieun and Song, Joon Woo and Seok, Jae Yeon and Kim, Pilhan and Kim, Jin Won and Yoo, Hongki},
  year = 2025,
  month = may,
  journal = {PhotoniX},
  volume = {6},
  number = {1},
  pages = {15},
  issn = {2662-1991},
  doi = {10.1186/s43074-025-00173-8},
  urldate = {2025-10-08},
  abstract = {Fluorescence microscopy has become one of the most widely employed in vivo imaging modalities, enabling the discovery of new biopathological mechanisms. However, the application of fluorescence imaging is often hindered by signal-to-noise ratio issues owing to inherent noise arising from various systemic and biophysical characteristics. These limitations pose a growing challenge, especially with the desire to elucidate dynamic biomechanisms at previously unreachable rapid speeds. Here, we propose a temporal gradient (TG)-based self-supervised denoising network (TeD) that could enable an unprecedented advance in spatially dynamic fluorescence imaging. Our strategy is predicated on the insight that judicious utilization of spatiotemporal information is more advantageous for denoising predictions. Adopting the TG, which intrinsically embodies spatial dynamic features, enables TeD to prudently focus on spatiotemporal information. We showed that TeD can provide new interpretative opportunities for understanding dynamic fluorescence signals in in vivo imaging of mice, representing cellular flow. Furthermore, we demonstrated that TeD is robust even when fluorescence signals exhibit temporal kinetics without spatial dynamics, as seen in neuronal population imaging. We believe that TeD's superior performance even with spatially dynamic samples, including the complex behavior of cells or organisms, could make a substantial contribution to various biological studies.},
  file = {/Users/yiyuanli/Zotero/storage/H42HEKV3/Lee et al. - 2025 - Self-supervised denoising of dynamic fluorescence images via temporal gradient-empowered deep learni.pdf}
}

@misc{lehtinenNoise2NoiseLearningImage2018,
  title = {{{Noise2Noise}}: {{Learning Image Restoration}} without {{Clean Data}}},
  shorttitle = {{{Noise2Noise}}},
  author = {Lehtinen, Jaakko and Munkberg, Jacob and Hasselgren, Jon and Laine, Samuli and Karras, Tero and Aittala, Miika and Aila, Timo},
  year = 2018,
  month = oct,
  number = {arXiv:1803.04189},
  eprint = {1803.04189},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1803.04189},
  urldate = {2025-10-08},
  abstract = {We apply basic statistical reasoning to signal reconstruction by machine learning -- learning to map corrupted observations to clean signals -- with a simple and powerful conclusion: it is possible to learn to restore images by only looking at corrupted examples, at performance at and sometimes exceeding training using clean data, without explicit image priors or likelihood models of the corruption. In practice, we show that a single model learns photographic noise removal, denoising synthetic Monte Carlo images, and reconstruction of undersampled MRI scans -- all corrupted by different processes -- based on noisy data only.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/yiyuanli/Zotero/storage/VEAXZ2UG/Lehtinen et al. - 2018 - Noise2Noise Learning Image Restoration without Clean Data.pdf}
}

@misc{liExploringImpactTemperature2025,
  title = {Exploring the {{Impact}} of {{Temperature}} on {{Large Language Models}}:{{Hot}} or {{Cold}}?},
  shorttitle = {Exploring the {{Impact}} of {{Temperature}} on {{Large Language Models}}},
  author = {Li, Lujun and Sleem, Lama and Gentile, Niccolo' and Nichil, Geoffrey and State, Radu},
  year = 2025,
  month = jun,
  number = {arXiv:2506.07295},
  eprint = {2506.07295},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.07295},
  urldate = {2025-10-06},
  abstract = {The sampling temperature, a critical hyperparameter in large language models (LLMs), modifies the logits before the softmax layer, thereby reshaping the distribution of output tokens. Recent studies have challenged the "Stochastic Parrots" analogy by demonstrating that LLMs are capable of understanding semantics rather than merely memorizing data and that randomness, modulated by sampling temperature, plays a crucial role in model inference. In this study, we systematically evaluated the impact of temperature in the range of 0 to 2 on data sets designed to assess six different capabilities, conducting statistical analyses on open source models of three different sizes. Small (1B-4B), medium (6B-13B), and large (40B-80B). Our findings reveal distinct skill-specific effects of temperature on model performance, highlighting the complexity of optimal temperature selection in practical applications. To address this challenge, we propose a BERT-based temperature selector that takes advantage of these observed effects to identify the optimal temperature for a given prompt. We demonstrate that this approach can significantly improve the performance of small and medium models in the SuperGLUE datasets. Furthermore, our study extends to FP16 precision inference, revealing that temperature effects are consistent with those observed in 4-bit quantized models. By evaluating temperature effects up to 4.0 in three quantized models, we find that the ``Mutation Temperature''---the point at which significant performance changes occur - increases with model size1.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/yiyuanli/Zotero/storage/RMC9LK6Y/Li et al. - 2025 - Exploring the Impact of Temperature on Large Language ModelsHot or Cold.pdf}
}

@misc{liSimpleVLARLScalingVLA2025,
  title = {{{SimpleVLA-RL}}: {{Scaling VLA Training}} via {{Reinforcement Learning}}},
  shorttitle = {{{SimpleVLA-RL}}},
  author = {Li, Haozhan and Zuo, Yuxin and Yu, Jiale and Zhang, Yuhao and Yang, Zhaohui and Zhang, Kaiyan and Zhu, Xuekai and Zhang, Yuchen and Chen, Tianxing and Cui, Ganqu and Wang, Dehui and Luo, Dingxiang and Fan, Yuchen and Sun, Youbang and Zeng, Jia and Pang, Jiangmiao and Zhang, Shanghang and Wang, Yu and Mu, Yao and Zhou, Bowen and Ding, Ning},
  year = 2025,
  month = sep,
  number = {arXiv:2509.09674},
  eprint = {2509.09674},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2509.09674},
  urldate = {2025-09-13},
  abstract = {Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation. Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges: (i) the scarcity and high cost of large-scale human-operated robotic trajectories required for SFT scaling, and (ii) limited generalization to tasks involving distribution shift. Recent breakthroughs in Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can dramatically enhance step-by-step reasoning capabilities, raising a natural question: Can RL similarly improve the long-horizon step-by-step action planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL framework tailored for VLA models. Building upon veRL, we introduce VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation. When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms {$\pi$}0 on RoboTwin 1.0\&2.0 with the exploration-enhancing strategies we introduce. SimpleVLA-RL not only reduces dependence on large-scale data and enables robust generalization, but also remarkably surpasses SFT in real-world tasks. Moreover, we identify a novel phenomenon ``pushcut'' during RL training, wherein the policy discovers previously unseen patterns beyond those seen in the previous training process.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/yiyuanli/Zotero/storage/WFQVRUWK/Li et al. - 2025 - SimpleVLA-RL Scaling VLA Training via Reinforcement Learning.pdf}
}

@misc{liuEmpiricalAnalysisLarge2024,
  title = {An {{Empirical Analysis}} on {{Large Language Models}} in {{Debate Evaluation}}},
  author = {Liu, Xinyi and Liu, Pinxin and He, Hangfeng},
  year = 2024,
  month = jun,
  number = {arXiv:2406.00050},
  eprint = {2406.00050},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.00050},
  urldate = {2025-09-22},
  abstract = {In this study, we investigate the capabilities and inherent biases of advanced large language models (LLMs) such as GPT-3.5 and GPT-4 in the context of debate evaluation. We discover that LLM's performance exceeds humans and surpasses the performance of state-of-the-art methods fine-tuned on extensive datasets in debate evaluation. We additionally explore and analyze biases present in LLMs, including positional bias, lexical bias, order bias, which may affect their evaluative judgments. Our findings reveal a consistent bias in both GPT-3.5 and GPT-4 towards the second candidate response presented, attributed to prompt design. We also uncover lexical biases in both GPT-3.5 and GPT-4, especially when label sets carry connotations such as numerical or sequential, highlighting the critical need for careful label verbalizer selection in prompt design. Additionally, our analysis indicates a tendency of both models to favor the debate's concluding side as the winner, suggesting an end-of-discussion bias.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/yiyuanli/Zotero/storage/UGUFJVBK/Liu et al. - 2024 - An Empirical Analysis on Large Language Models in Debate Evaluation.pdf}
}

@book{liuHedongXianSheng1965,
  title = {{Hedong xian sheng long cheng lu: [shang xia juan]}},
  shorttitle = {{Hedong xian sheng long cheng lu}},
  author = {Liu, Zongyuan and {柳宗元}},
  year = 1965,
  series = {{Bai chuan xue hai ; di 2 han}},
  publisher = {Yi wen yin shu guan},
  address = {Taibei},
  abstract = {Series statement: Bai chuan xue hai ; di 2 han, Series statement: Bai bu cong shu ji cheng ; 2, On double leaves, traditional East Asian style binding, in case.},
  langid = {chi},
  keywords = {Fantasy fiction Chinese}
}

@misc{liuPartTricksTraps2025,
  title = {Part {{I}}: {{Tricks}} or {{Traps}}? {{A Deep Dive}} into {{RL}} for {{LLM Reasoning}}},
  shorttitle = {Part {{I}}},
  author = {Liu, Zihe and Liu, Jiashun and He, Yancheng and Wang, Weixun and Liu, Jiaheng and Pan, Ling and Hu, Xinyu and Xiong, Shaopan and Huang, Ju and Hu, Jian and Huang, Shengyi and Yang, Siran and Wang, Jiamang and Su, Wenbo and Zheng, Bo},
  year = 2025,
  month = aug,
  number = {arXiv:2508.08221},
  eprint = {2508.08221},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2508.08221},
  urldate = {2025-08-30},
  abstract = {Reinforcement learning for LLM reasoning has rapidly emerged as a prominent research area, marked by a significant surge in related studies on both algorithmic innovations and practical applications. Despite this progress, several critical challenges remain, including the absence of standardized guidelines for employing RL techniques and a fragmented understanding of their underlying mechanisms. Additionally, inconsistent experimental settings, variations in training data, and differences in model initialization have led to conflicting conclusions, obscuring the key characteristics of these techniques and creating confusion among practitioners when selecting appropriate techniques. This paper systematically reviews widely adopted RL techniques through rigorous reproductions and isolated evaluations within a unified open-source framework. We analyze the internal mechanisms, applicable scenarios, and core principles of each technique through fine-grained experiments, including datasets of varying difficulty, model sizes, and architectures. Based on these insights, we present clear guidelines for selecting RL techniques tailored to specific setups, and provide a reliable roadmap for practitioners navigating the RL for the LLM domain. Finally, we reveal that a minimalist combination of two techniques can unlock the learning capability of critic-free policies using vanilla PPO loss. The results demonstrate that our simple combination consistently improves performance, surpassing strategies like GRPO and DAPO.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/yiyuanli/Zotero/storage/SQPTS8T6/Liu et al. - 2025 - Part I Tricks or Traps A Deep Dive into RL for LLM Reasoning.pdf}
}

@misc{longoMultiuserCommunicationSystems1981,
  title = {Multi-User Communication Systems},
  author = {Longo, G. and {International Centre for Mechanical Sciences}},
  year = 1981,
  journal = {Multi-user communication systems},
  series = {Courses and Lectures / {{International Centre}} for {{Mechanical Sciences}} ; No. 265},
  publisher = {Springer},
  address = {Wien ;},
  abstract = {Includes bibliographical references.},
  isbn = {9783211814840},
  langid = {english},
  keywords = {Multichannel communication,Telecommunication systems}
}

@misc{luparelloTwoLevelNested2025,
  title = {Two {{Level Nested}} and {{Sequential Logit}}},
  author = {Luparello, Davide},
  year = 2025,
  month = mar,
  number = {arXiv:2503.21808},
  eprint = {2503.21808},
  primaryclass = {econ},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.21808},
  urldate = {2025-09-22},
  abstract = {This technical note provides comprehensive derivations of fundamental equations in two-level nested and sequential logit models for analyzing hierarchical choice structures. We present derivations of the Berry (1994) inversion formula, nested inclusive values computation, and multi-level market share equations, complementing existing literature. While conceptually distinct, nested and sequential logit models share mathematical similarities and, under specific distributional assumptions, yield identical inversion formulas---offering valuable analytical insights. These notes serve as a practical reference for researchers implementing multi-level discrete choice models in empirical applications, particularly in industrial organization and demand estimation contexts, and complement Mansley et al. (2019).},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Economics - Econometrics,Economics - General Economics,Quantitative Finance - Economics},
  file = {/Users/yiyuanli/Zotero/storage/JM8GWVNI/Luparello - 2025 - Two Level Nested and Sequential Logit.pdf}
}

@misc{mackayInformationTheoryInference2004,
  title = {Information Theory, Inference, and Learning Algorithms},
  author = {MacKay, David J. C.},
  year = 2004,
  journal = {Information theory, inference, and learning algorithms},
  edition = {Reprinted with corrections.},
  publisher = {Cambridge University Press},
  address = {Cambridge, United Kingdom ;},
  abstract = {Includes bibliographical references (pages 613-619) and index.},
  isbn = {9780521642989},
  langid = {english},
  keywords = {Information theory}
}

@misc{madaanQuantifyingVarianceEvaluation2024,
  title = {Quantifying {{Variance}} in {{Evaluation Benchmarks}}},
  author = {Madaan, Lovish and Singh, Aaditya K. and Schaeffer, Rylan and Poulton, Andrew and Koyejo, Sanmi and Stenetorp, Pontus and Narang, Sharan and Hupkes, Dieuwke},
  year = 2024,
  month = jun,
  number = {arXiv:2406.10229},
  eprint = {2406.10229},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.10229},
  urldate = {2025-08-30},
  abstract = {Evaluation benchmarks are the cornerstone of measuring capabilities of large language models (LLMs), as well as driving progress in said capabilities. Originally designed to make claims about capabilities (or lack thereof) in fully pretrained models, evaluation benchmarks are now also extensively used to decide between various training choices. Despite this widespread usage, we rarely quantify the variance in our evaluation benchmarks, which dictates whether differences in performance are meaningful. Here, we define and measure a range of metrics geared towards measuring variance in evaluation benchmarks, including seed variance across initialisations, and monotonicity during training. By studying a large number of models -- both openly available and pretrained from scratch -- we provide empirical estimates for a variety of variance metrics, with considerations and recommendations for practitioners. We also evaluate the utility and tradeoffs of continuous versus discrete performance measures and explore options for better understanding and reducing this variance. We find that simple changes, such as framing choice tasks (like MMLU) as completion tasks, can often reduce variance for smaller scale ({$\sim$}7B) models, while more involved methods inspired from human testing literature (such as item analysis and item response theory) struggle to meaningfully reduce variance. Overall, our work provides insights into variance in evaluation benchmarks, suggests LM-specific techniques to reduce variance, and more generally encourages practitioners to carefully factor in variance when comparing models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/yiyuanli/Zotero/storage/DRLY4ZL7/Madaan et al. - 2024 - Quantifying Variance in Evaluation Benchmarks.pdf}
}

@misc{mazzolaImplementingEloRating2020,
  title = {Implementing the {{Elo Rating System}}},
  author = {Mazzola, Matt},
  year = 2020,
  month = nov,
  journal = {Medium},
  urldate = {2025-09-22},
  abstract = {In this article we'll look at how to implement the Elo rating system in code.},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/6NVAQKCV/implementing-the-elo-rating-system-a085f178e065.html}
}

@article{mcintoshInadequaciesLargeLanguage2025,
  title = {Inadequacies of {{Large Language Model Benchmarks}} in the {{Era}} of {{Generative Artificial Intelligence}}},
  author = {McIntosh, Timothy R. and Susnjak, Teo and Arachchilage, Nalin and Liu, Tong and Watters, Paul and Halgamuge, Malka N.},
  year = 2025,
  journal = {IEEE Transactions on Artificial Intelligence},
  eprint = {2402.09880},
  primaryclass = {cs},
  pages = {1--18},
  issn = {2691-4581},
  doi = {10.1109/TAI.2025.3569516},
  urldate = {2025-09-22},
  abstract = {The rapid rise in popularity of Large Language Models (LLMs) with emerging capabilities has spurred public curiosity to evaluate and compare different LLMs, leading many researchers to propose their own LLM benchmarks. Noticing preliminary inadequacies in those benchmarks, we embarked on a study to critically assess 23 state-of-the-art LLM benchmarks, using our novel unified evaluation framework through the lenses of people, process, and technology, under the pillars of benchmark functionality and integrity. Our research uncovered significant limitations, including biases, difficulties in measuring genuine reasoning, adaptability, implementation inconsistencies, prompt engineering complexity, evaluator diversity, and the overlooking of cultural and ideological norms in one comprehensive assessment. Our discussions emphasized the urgent need for standardized methodologies, regulatory certainties, and ethical guidelines in light of Artificial Intelligence (AI) advancements, including advocating for an evolution from static benchmarks to dynamic behavioral profiling to accurately capture LLMs' complex behaviors and potential risks. Our study highlighted the necessity for a paradigm shift in LLM evaluation methodologies, underlining the importance of collaborative efforts for the development of universally accepted benchmarks and the enhancement of AI systems' integration into society.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction},
  file = {/Users/yiyuanli/Zotero/storage/8CF226LH/McIntosh et al. - 2025 - Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence.pdf}
}

@misc{millerAddingErrorBars2024,
  title = {Adding {{Error Bars}} to {{Evals}}: {{A Statistical Approach}} to {{Language Model Evaluations}}},
  shorttitle = {Adding {{Error Bars}} to {{Evals}}},
  author = {Miller, Evan},
  year = 2024,
  month = nov,
  number = {arXiv:2411.00640},
  eprint = {2411.00640},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.00640},
  urldate = {2025-10-05},
  abstract = {Evaluations are critical for understanding the capabilities of large language models (LLMs). Fundamentally, evaluations are experiments; but the literature on evaluations has largely ignored the literature from other sciences on experiment analysis and planning. This article shows researchers with some training in statistics how to think about and analyze data from language model evaluations. Conceptualizing evaluation questions as having been drawn from an unseen super-population, we present formulas for analyzing evaluation data, measuring differences between two models, and planning an evaluation experiment. We make a number of specific recommendations for running language model evaluations and reporting experiment results in a way that minimizes statistical noise and maximizes informativeness.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Statistics - Applications},
  file = {/Users/yiyuanli/Zotero/storage/AU5KKA8G/Miller - 2024 - Adding Error Bars to Evals A Statistical Approach to Language Model Evaluations.pdf}
}

@article{milnerHowWeighDonkey2014,
  title = {How to Weigh a Donkey in the {{Kenyan}} Countryside},
  author = {Milner, Kate and Rougier, Jonathan},
  year = 2014,
  journal = {Significance},
  volume = {11},
  number = {4},
  pages = {40--43},
  issn = {1740-9713},
  doi = {10.1111/j.1740-9713.2014.00768.x},
  urldate = {2025-09-01},
  abstract = {Donkeys play a crucial role in the lives of rural Kenyans. When they fall sick, vets need a quick and accurate method of weighing the animals to administer the right dosage of drugs. The humble nomogram can help, as Kate Milner and Jonathan Rougier explain.},
  copyright = {\copyright{} 2014 The Royal Statistical Society},
  file = {/Users/yiyuanli/Zotero/storage/M9T7NWVF/Milner and Rougier - 2014 - How to weigh a donkey in the Kenyan countryside.pdf}
}

@misc{minaeeLargeLanguageModels2025,
  title = {Large {{Language Models}}: {{A Survey}}},
  shorttitle = {Large {{Language Models}}},
  author = {Minaee, Shervin and Mikolov, Tomas and Nikzad, Narjes and Chenaghlu, Meysam and Socher, Richard and Amatriain, Xavier and Gao, Jianfeng},
  year = 2025,
  month = mar,
  number = {arXiv:2402.06196},
  eprint = {2402.06196},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.06196},
  urldate = {2025-12-18},
  abstract = {Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws \textbackslash cite\textbraceleft kaplan2020scaling,hoffmann2022training\textbraceright. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/yiyuanli/Zotero/storage/Z6YNIEEV/Minaee et al. - 2025 - Large Language Models A Survey.pdf;/Users/yiyuanli/Zotero/storage/SA9T8689/2402.html}
}

@misc{moserStudentsGuideCoding2012,
  title = {A Student's Guide to Coding and Information Theory},
  author = {Moser, Stefan M.},
  year = 2012,
  journal = {A student's guide to coding and information theory},
  publisher = {Cambridge University Press},
  address = {Cambridge ;},
  abstract = {Includes bibliographical references and index.},
  collaborator = {Chen, Po-Ning},
  isbn = {9781107015838},
  langid = {english},
  keywords = {Coding theory,Information theory}
}

@misc{niSurveyLargeLanguage2025,
  title = {A {{Survey}} on {{Large Language Model Benchmarks}}},
  author = {Ni, Shiwen and Chen, Guhong and Li, Shuaimin and Chen, Xuanang and Li, Siyi and Wang, Bingli and Wang, Qiyao and Wang, Xingjian and Zhang, Yifan and Fan, Liyang and Li, Chengming and Xu, Ruifeng and Sun, Le and Yang, Min},
  year = 2025,
  month = aug,
  number = {arXiv:2508.15361},
  eprint = {2508.15361},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2508.15361},
  urldate = {2025-10-05},
  abstract = {In recent years, with the rapid development of the depth and breadth of large language models' capabilities, various corresponding evaluation benchmarks have been emerging in increasing numbers. As a quantitative assessment tool for model performance, benchmarks are not only a core means to measure model capabilities but also a key element in guiding the direction of model development and promoting technological innovation. We systematically review the current status and development of large language model benchmarks for the first time, categorizing 283 representative benchmarks into three categories: general capabilities, domain-specific, and target-specific. General capability benchmarks cover aspects such as core linguistics, knowledge, and reasoning; domain-specific benchmarks focus on fields like natural sciences, humanities and social sciences, and engineering technology; target-specific benchmarks pay attention to risks, reliability, agents, etc. We point out that current benchmarks have problems such as inflated scores caused by data contamination, unfair evaluation due to cultural and linguistic biases, and lack of evaluation on process credibility and dynamic environments, and provide a referable design paradigm for future benchmark innovation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/yiyuanli/Zotero/storage/4W2PXS36/Ni et al. - 2025 - A Survey on Large Language Model Benchmarks.pdf;/Users/yiyuanli/Zotero/storage/DRD8WKGA/2508.html}
}

@misc{openaiWhyLanguageModels2025,
  type = {Why Language Models Hallucinate},
  title = {Why Language Models Hallucinate},
  author = {OPENAI},
  year = 2025,
  month = sep,
  urldate = {2025-09-22},
  abstract = {OpenAI's new research explains why language models hallucinate. The findings show how improved evaluations can enhance AI reliability, honesty, and safety.},
  howpublished = {https://openai.com/index/why-language-models-hallucinate/},
  langid = {american},
  file = {/Users/yiyuanli/Zotero/storage/7IMJG2M2/why-language-models-hallucinate.html}
}

@article{pearsCorrelatesCharacteristicsAmerican2022,
  title = {The {{Correlates}} and {{Characteristics}} of {{American State Identity}}},
  author = {Pears, Emily and Sydnor, Emily},
  year = 2022,
  month = apr,
  journal = {Publius: The Journal of Federalism},
  volume = {52},
  number = {2},
  pages = {173--200},
  issn = {0048-5950},
  doi = {10.1093/publius/pjac004},
  urldate = {2024-01-25},
  abstract = {The Federalist Papers highlight the role that citizens' state identities will play in American federalism, yet some scholars argue that contemporary Americans have shed their state attachments. Drawing on data from a nationally representative survey, we demonstrate that individuals still hold dual national and state identities, and that the likelihood that one will feel attached to their state depends on a variety of individual characteristics such as education, identification with a marginalized or minority community within the state, and one's ideological ``fit'' with the partisan majority in their state, leading to significant variance from one citizen to the next. Additionally, we find that this state identity is correlated with political attitudes, particularly trust in and assessment of state elected officials. Individuals who hold stronger state identities are also more likely to trust their state government. These findings have implications for our understanding of the dynamics of federalism in modern U.S. politics.},
  file = {/Users/yiyuanli/Zotero/storage/QN46ABSK/Pears and Sydnor - 2022 - The Correlates and Characteristics of American Sta.pdf}
}

@misc{phanHumanitysLastExam2025,
  title = {Humanity's {{Last Exam}}},
  author = {Phan, Long and Gatti, Alice and Han, Ziwen and Li, Nathaniel and Hu, Josephina and Zhang, Hugh and Zhang, Chen Bo Calvin and Shaaban, Mohamed and Ling, John and Shi, Sean and Choi, Michael and Agrawal, Anish and Chopra, Arnav and Khoja, Adam and Kim, Ryan and Ren, Richard and Hausenloy, Jason and Zhang, Oliver and Mazeika, Mantas and Dodonov, Dmitry and Nguyen, Tung and Lee, Jaeho and Anderson, Daron and Doroshenko, Mikhail and Stokes, Alun Cennyth and Mahmood, Mobeen and Pokutnyi, Oleksandr and Iskra, Oleg and Wang, Jessica P. and Levin, John-Clark and Kazakov, Mstyslav and Feng, Fiona and Feng, Steven Y. and Zhao, Haoran and Yu, Michael and Gangal, Varun and Zou, Chelsea and Wang, Zihan and Popov, Serguei and Gerbicz, Robert and Galgon, Geoff and Schmitt, Johannes and Yeadon, Will and Lee, Yongki and Sauers, Scott and Sanchez, Alvaro and Giska, Fabian and Roth, Marc and Riis, S{\o}ren and Utpala, Saiteja and Burns, Noah and Goshu, Gashaw M. and Naiya, Mohinder Maheshbhai and Agu, Chidozie and Giboney, Zachary and Cheatom, Antrell and {Fournier-Facio}, Francesco and Crowson, Sarah-Jane and Finke, Lennart and Cheng, Zerui and Zampese, Jennifer and Hoerr, Ryan G. and Nandor, Mark and Park, Hyunwoo and Gehrunger, Tim and Cai, Jiaqi and McCarty, Ben and Garretson, Alexis C. and Taylor, Edwin and Sileo, Damien and Ren, Qiuyu and Qazi, Usman and Li, Lianghui and Nam, Jungbae and Wydallis, John B. and Arkhipov, Pavel and Shi, Jack Wei Lun and Bacho, Aras and Willcocks, Chris G. and Cao, Hangrui and Motwani, Sumeet and Santos, Emily de Oliveira and Veith, Johannes and Vendrow, Edward and Cojoc, Doru and Zenitani, Kengo and Robinson, Joshua and Tang, Longke and Li, Yuqi and Vendrow, Joshua and Fraga, Natanael Wildner and Kuchkin, Vladyslav and Maksimov, Andrey Pupasov and Marion, Pierre and Efremov, Denis and Lynch, Jayson and Liang, Kaiqu and Mikov, Aleksandar and Gritsevskiy, Andrew and Guillod, Julien and Demir, G{\"o}zdenur and Martinez, Dakotah and Pageler, Ben and Zhou, Kevin and Soori, Saeed and Press, Ori and Tang, Henry and Rissone, Paolo and Green, Sean R. and Br{\"u}ssel, Lina and Twayana, Moon and Dieuleveut, Aymeric and Imperial, Joseph Marvin and Prabhu, Ameya and Yang, Jinzhou and Crispino, Nick and Rao, Arun and Zvonkine, Dimitri and Loiseau, Gabriel and Kalinin, Mikhail and Lukas, Marco and Manolescu, Ciprian and Stambaugh, Nate and Mishra, Subrata and Hogg, Tad and Bosio, Carlo and Coppola, Brian P. and Salazar, Julian and Jin, Jaehyeok and Sayous, Rafael and Ivanov, Stefan and Schwaller, Philippe and Senthilkuma, Shaipranesh and Bran, Andres M. and Algaba, Andres and den Houte, Kelsey Van and Sypt, Lynn Van Der and Verbeken, Brecht and Noever, David and Kopylov, Alexei and Myklebust, Benjamin and Li, Bikun and Schut, Lisa and Zheltonozhskii, Evgenii and Yuan, Qiaochu and Lim, Derek and Stanley, Richard and Yang, Tong and Maar, John and Wykowski, Julian and Oller, Mart{\'i} and Sahu, Anmol and Ardito, Cesare Giulio and Hu, Yuzheng and Kamdoum, Ariel Ghislain Kemogne and Jin, Alvin and Vilchis, Tobias Garcia and Zu, Yuexuan and Lackner, Martin and Koppel, James and Sun, Gongbo and Antonenko, Daniil S. and Chern, Steffi and Zhao, Bingchen and Arsene, Pierrot and Cavanagh, Joseph M. and Li, Daofeng and Shen, Jiawei and Crisostomi, Donato and Zhang, Wenjin and Dehghan, Ali and Ivanov, Sergey and Perrella, David and Kaparov, Nurdin and Zang, Allen and Sucholutsky, Ilia and Kharlamova, Arina and Orel, Daniil and Poritski, Vladislav and {Ben-David}, Shalev and Berger, Zachary and Whitfill, Parker and Foster, Michael and Munro, Daniel and Ho, Linh and Sivarajan, Shankar and Hava, Dan Bar and Kuchkin, Aleksey and Holmes, David and {Rodriguez-Romero}, Alexandra and Sommerhage, Frank and Zhang, Anji and Moat, Richard and Schneider, Keith and Kazibwe, Zakayo and Clarke, Don and Kim, Dae Hyun and Dias, Felipe Meneguitti and Fish, Sara and Elser, Veit and Kreiman, Tobias and Vilchis, Victor Efren Guadarrama and Klose, Immo and Anantheswaran, Ujjwala and Zweiger, Adam and Rawal, Kaivalya and Li, Jeffery and Nguyen, Jeremy and Daans, Nicolas and Heidinger, Haline and Radionov, Maksim and Rozho{\v n}, V{\'a}clav and Ginis, Vincent and Stump, Christian and Cohen, Niv and Po{\'s}wiata, Rafa{\l} and Tkadlec, Josef and Goldfarb, Alan and Wang, Chenguang and Padlewski, Piotr and Barzowski, Stanislaw and Montgomery, Kyle and Stendall, Ryan and {Tucker-Foltz}, Jamie and Stade, Jack and Rogers, T. Ryan and Goertzen, Tom and Grabb, Declan and Shukla, Abhishek and Givr{\'e}, Alan and Ambay, John Arnold and Sen, Archan and Aziz, Muhammad Fayez and Inlow, Mark H. and He, Hao and Zhang, Ling and Kaddar, Younesse and {\"A}ngquist, Ivar and Chen, Yanxu and Wang, Harrison K. and Ramakrishnan, Kalyan and Thornley, Elliott and Terpin, Antonio and Schoelkopf, Hailey and Zheng, Eric and Carmi, Avishy and Brown, Ethan D. L. and Zhu, Kelin and Bartolo, Max and Wheeler, Richard and Stehberger, Martin and Bradshaw, Peter and Heimonen, J. P. and Sridhar, Kaustubh and Akov, Ido and Sandlin, Jennifer and Makarychev, Yury and Tam, Joanna and Hoang, Hieu and Cunningham, David M. and Goryachev, Vladimir and Patramanis, Demosthenes and Krause, Michael and Redenti, Andrew and Aldous, David and Lai, Jesyin and Coleman, Shannon and Xu, Jiangnan and Lee, Sangwon and Magoulas, Ilias and Zhao, Sandy and Tang, Ning and Cohen, Michael K. and Paradise, Orr and Kirchner, Jan Hendrik and Ovchynnikov, Maksym and Matos, Jason O. and Shenoy, Adithya and Wang, Michael and Nie, Yuzhou and {Sztyber-Betley}, Anna and Faraboschi, Paolo and Riblet, Robin and Crozier, Jonathan and Halasyamani, Shiv and Verma, Shreyas and Joshi, Prashant and Meril, Eli and Ma, Ziqiao and Andr{\'e}oletti, J{\'e}r{\'e}my and Singhal, Raghav and Platnick, Jacob and Nevirkovets, Volodymyr and Basler, Luke and Ivanov, Alexander and Khoury, Seri and Gustafsson, Nils and Piccardo, Marco and Mostaghimi, Hamid and Chen, Qijia and Singh, Virendra and Kh{\'a}nh, Tran Quoc and Rosu, Paul and Szlyk, Hannah and Brown, Zachary and Narayan, Himanshu and Menezes, Aline and Roberts, Jonathan and Alley, William and Sun, Kunyang and Patel, Arkil and Lamparth, Max and Reuel, Anka and Xin, Linwei and Xu, Hanmeng and Loader, Jacob and Martin, Freddie and Wang, Zixuan and Achilleos, Andrea and Preu, Thomas and Korbak, Tomek and Bosio, Ida and Kazemi, Fereshteh and Chen, Ziye and B{\'a}lint, Bir{\'o} and Lo, Eve J. Y. and Wang, Jiaqi and Nunes, Maria In{\^e}s S. and Milbauer, Jeremiah and Bari, M. Saiful and Wang, Zihao and Ansarinejad, Behzad and Sun, Yewen and Durand, Stephane and Elgnainy, Hossam and Douville, Guillaume and Tordera, Daniel and Balabanian, George and Wolff, Hew and Kvistad, Lynna and Milliron, Hsiaoyun and Sakor, Ahmad and Eron, Murat and O, Andrew Favre D. and Shah, Shailesh and Zhou, Xiaoxiang and Kamalov, Firuz and Abdoli, Sherwin and Santens, Tim and Barkan, Shaul and Tee, Allison and Zhang, Robin and Tomasiello, Alessandro and Luca, G. Bruno De and Looi, Shi-Zhuo and Le, Vinh-Kha and Kolt, Noam and Pan, Jiayi and Rodman, Emma and Drori, Jacob and Fossum, Carl J. and Muennighoff, Niklas and Jagota, Milind and Pradeep, Ronak and Fan, Honglu and Eicher, Jonathan and Chen, Michael and Thaman, Kushal and Merrill, William and Firsching, Moritz and Harris, Carter and Ciob{\^a}c{\u a}, Stefan and Gross, Jason and Pandey, Rohan and Gusev, Ilya and Jones, Adam and Agnihotri, Shashank and Zhelnov, Pavel and Mofayezi, Mohammadreza and Piperski, Alexander and Zhang, David K. and Dobarskyi, Kostiantyn and Leventov, Roman and Soroko, Ignat and Duersch, Joshua and Taamazyan, Vage and Ho, Andrew and Ma, Wenjie and Held, William and Xian, Ruicheng and Zebaze, Armel Randy and Mohamed, Mohanad and Leser, Julian Noah and Yuan, Michelle X. and Yacar, Laila and Lengler, Johannes and Olszewska, Katarzyna and Fratta, Claudio Di and Oliveira, Edson and Jackson, Joseph W. and Zou, Andy and Chidambaram, Muthu and Manik, Timothy and Haffenden, Hector and Stander, Dashiell and Dasouqi, Ali and Shen, Alexander and Golshani, Bita and Stap, David and Kretov, Egor and Uzhou, Mikalai and Zhidkovskaya, Alina Borisovna and Winter, Nick and Rodriguez, Miguel Orbegozo and Lauff, Robert and Wehr, Dustin and Tang, Colin and Hossain, Zaki and Phillips, Shaun and Samuele, Fortuna and Ekstr{\"o}m, Fredrik and Hammon, Angela and Patel, Oam and Farhidi, Faraz and Medley, George and Mohammadzadeh, Forough and Pe{\~n}aflor, Madellene and Kassahun, Haile and Friedrich, Alena and Perez, Rayner Hernandez and Pyda, Daniel and Sakal, Taom and Dhamane, Omkar and Mirabadi, Ali Khajegili and Hallman, Eric and Okutsu, Kenchi and Battaglia, Mike and Maghsoudimehrabani, Mohammad and Amit, Alon and Hulbert, Dave and Pereira, Roberto and Weber, Simon and Handoko and Peristyy, Anton and Malina, Stephen and Mehkary, Mustafa and Aly, Rami and Reidegeld, Frank and Dick, Anna-Katharina and Friday, Cary and Singh, Mukhwinder and Shapourian, Hassan and Kim, Wanyoung and Costa, Mariana and Gurdogan, Hubeyb and Kumar, Harsh and Ceconello, Chiara and Zhuang, Chao and Park, Haon and Carroll, Micah and Tawfeek, Andrew R. and Steinerberger, Stefan and Aggarwal, Daattavya and Kirchhof, Michael and Dai, Linjie and Kim, Evan and Ferret, Johan and Shah, Jainam and Wang, Yuzhou and Yan, Minghao and Burdzy, Krzysztof and Zhang, Lixin and Franca, Antonio and Pham, Diana T. and Loh, Kang Yong and Robinson, Joshua and Jackson, Abram and Giordano, Paolo and Petersen, Philipp and Cosma, Adrian and Colino, Jesus and White, Colin and Votava, Jacob and Vinnikov, Vladimir and Delaney, Ethan and Spelda, Petr and Stritecky, Vit and Shahid, Syed M. and Mourrat, Jean-Christophe and Vetoshkin, Lavr and Sponselee, Koen and Bacho, Renas and Yong, Zheng-Xin and de la Rosa, Florencia and Cho, Nathan and Li, Xiuyu and Malod, Guillaume and Weller, Orion and Albani, Guglielmo and Lang, Leon and Laurendeau, Julien and Kazakov, Dmitry and Adesanya, Fatimah and Portier, Julien and Hollom, Lawrence and Souza, Victor and Zhou, Yuchen Anna and Degorre, Julien and Yal{\i}n, Yi{\u g}it and Obikoya, Gbenga Daniel and Rai and Bigi, Filippo and Bosc{\'a}, M. C. and Shumar, Oleg and Bacho, Kaniuar and Recchia, Gabriel and Popescu, Mara and Shulga, Nikita and Tanwie, Ngefor Mildred and Lux, Thomas C. H. and Rank, Ben and Ni, Colin and Brooks, Matthew and Yakimchyk, Alesia and Huanxu and Liu and Cavalleri, Stefano and H{\"a}ggstr{\"o}m, Olle and Verkama, Emil and Newbould, Joshua and Gundlach, Hans and {Brito-Santana}, Leonor and Amaro, Brian and Vajipey, Vivek and Grover, Rynaa and Wang, Ting and Kratish, Yosi and Li, Wen-Ding and Gopi, Sivakanth and Caciolai, Andrea and de Witt, Christian Schroeder and {Hern{\'a}ndez-C{\'a}mara}, Pablo and Rodol{\`a}, Emanuele and Robins, Jules and Williamson, Dominic and Cheng, Vincent and Raynor, Brad and Qi, Hao and Segev, Ben and Fan, Jingxuan and Martinson, Sarah and Wang, Erik Y. and Hausknecht, Kaylie and Brenner, Michael P. and Mao, Mao and Demian, Christoph and Kassani, Peyman and Zhang, Xinyu and Avagian, David and Scipio, Eshawn Jessica and Ragoler, Alon and Tan, Justin and Sims, Blake and Plecnik, Rebeka and Kirtland, Aaron and Bodur, Omer Faruk and Shinde, D. P. and Labrador, Yan Carlos Leyva and Adoul, Zahra and Zekry, Mohamed and Karakoc, Ali and Santos, Tania C. B. and Shamseldeen, Samir and Karim, Loukmane and Liakhovitskaia, Anna and Resman, Nate and Farina, Nicholas and Gonzalez, Juan Carlos and Maayan, Gabe and Anderson, Earth and Pena, Rodrigo De Oliveira and Kelley, Elizabeth and Mariji, Hodjat and Pouriamanesh, Rasoul and Wu, Wentao and Finocchio, Ross and Alarab, Ismail and Cole, Joshua and Ferreira, Danyelle and Johnson, Bryan and Safdari, Mohammad and Dai, Liangti and Arthornthurasuk, Siriphan and McAlister, Isaac C. and Moyano, Alejandro Jos{\'e} and Pronin, Alexey and Fan, Jing and {Ramirez-Trinidad}, Angel and Malysheva, Yana and Pottmaier, Daphiny and Taheri, Omid and Stepanic, Stanley and Perry, Samuel and Askew, Luke and Rodr{\'i}guez, Ra{\'u}l Adri{\'a}n Huerta and Minissi, Ali M. R. and Lorena, Ricardo and Iyer, Krishnamurthy and Fasiludeen, Arshad Anil and Clark, Ronald and Ducey, Josh and Piza, Matheus and Somrak, Maja and Vergo, Eric and Qin, Juehang and Borb{\'a}s, Benj{\'a}min and Chu, Eric and Lindsey, Jack and Jallon, Antoine and McInnis, I. M. J. and Chen, Evan and Semler, Avi and Gloor, Luk and Shah, Tej and Carauleanu, Marc and Lauer, Pascal and Huy, Tran {\DJ}uc and Shahrtash, Hossein and Duc, Emilien and Lewark, Lukas and Brown, Assaf and Albanie, Samuel and Weber, Brian and Vaz, Warren S. and Clavier, Pierre and Fan, Yiyang and e Silva, Gabriel Poesia Reis and Long and Lian and Abramovitch, Marcus and Jiang, Xi and Mendoza, Sandra and Islam, Murat and Gonzalez, Juan and Mavroudis, Vasilios and Xu, Justin and Kumar, Pawan and Goswami, Laxman Prasad and Bugas, Daniel and Heydari, Nasser and Jeanplong, Ferenc and Jansen, Thorben and Pinto, Antonella and Apronti, Archimedes and Galal, Abdallah and {Ze-An}, Ng and Singh, Ankit and Jiang, Tong and Xavier, Joan of Arc and Agarwal, Kanu Priya and Berkani, Mohammed and Zhang, Gang and Du, Zhehang and Junior, Benedito Alves de Oliveira and Malishev, Dmitry and Remy, Nicolas and Hartman, Taylor D. and Tarver, Tim and Mensah, Stephen and Loume, Gautier Abou and Morak, Wiktor and Habibi, Farzad and Hoback, Sarah and Cai, Will and Gimenez, Javier and Montecillo, Roselynn Grace and {\L}ucki, Jakub and Campbell, Russell and Sharma, Asankhaya and Meer, Khalida and Gul, Shreen and Gonzalez, Daniel Espinosa and Alapont, Xavier and Hoover, Alex and Chhablani, Gunjan and Vargus, Freddie and Agarwal, Arunim and Jiang, Yibo and Patil, Deepakkumar and Outevsky, David and Scaria, Kevin Joseph and Maheshwari, Rajat and Dendane, Abdelkader and Shukla, Priti and Cartwright, Ashley and Bogdanov, Sergei and M{\"u}ndler, Niels and M{\"o}ller, S{\"o}ren and Arnaboldi, Luca and Thaman, Kunvar and Siddiqi, Muhammad Rehan and Saxena, Prajvi and Gupta, Himanshu and Fruhauff, Tony and Sherman, Glen and Vincze, M{\'a}ty{\'a}s and Usawasutsakorn, Siranut and Ler, Dylan and Radhakrishnan, Anil and Enyekwe, Innocent and Salauddin, Sk Md and Muzhen, Jiang and Maksapetyan, Aleksandr and Rossbach, Vivien and Harjadi, Chris and Bahaloohoreh, Mohsen and Sparrow, Claire and Sidhu, Jasdeep and Ali, Sam and Bian, Song and Lai, John and Singer, Eric and Uro, Justine Leon and Bateman, Greg and Sayed, Mohamed and Menshawy, Ahmed and Duclosel, Darling and Bezzi, Dario and Jain, Yashaswini and Aaron, Ashley and Tiryakioglu, Murat and Siddh, Sheeshram and Krenek, Keith and Shah, Imad Ali and Jin, Jun and Creighton, Scott and Peskoff, Denis and {EL-Wasif}, Zienab and V, Ragavendran P. and Richmond, Michael and McGowan, Joseph and Patwardhan, Tejal and Sun, Hao-Yu and Sun, Ting and Zubi{\'c}, Nikola and Sala, Samuele and Ebert, Stephen and Kaddour, Jean and Schottdorf, Manuel and Wang, Dianzhuo and Petruzella, Gerol and Meiburg, Alex and Medved, Tilen and ElSheikh, Ali and Hebbar, S. Ashwin and Vaquero, Lorenzo and Yang, Xianjun and Poulos, Jason and Zouhar, Vil{\'e}m and Bogdanik, Sergey and Zhang, Mingfang and {Sanz-Ros}, Jorge and Anugraha, David and Dai, Yinwei and Nhu, Anh N. and Wang, Xue and Demircali, Ali Anil and Jia, Zhibai and Zhou, Yuyin and Wu, Juncheng and He, Mike and Chandok, Nitin and Sinha, Aarush and Luo, Gaoxiang and Le, Long and Noy{\'e}, Micka{\"e}l and Pere{\l}kiewicz, Micha{\l} and Pantidis, Ioannis and Qi, Tianbo and Purohit, Soham Sachin and Parcalabescu, Letitia and Nguyen, Thai-Hoa and Winata, Genta Indra and Ponti, Edoardo M. and Li, Hanchen and Dhole, Kaustubh and Park, Jongee and Abbondanza, Dario and Wang, Yuanli and Nayak, Anupam and Caetano, Diogo M. and Wong, Antonio A. W. L. and del {Rio-Chanona}, Maria and Kondor, D{\'a}niel and Francois, Pieter and Chalstrey, Ed and Zsambok, Jakob and Hoyer, Dan and Reddish, Jenny and Hauser, Jakob and {Rodrigo-Gin{\'e}s}, Francisco-Javier and Datta, Suchandra and Shepherd, Maxwell and Kamphuis, Thom and Zhang, Qizheng and Kim, Hyunjun and Sun, Ruiji and Yao, Jianzhu and Dernoncourt, Franck and Krishna, Satyapriya and Rismanchian, Sina and Pu, Bonan and Pinto, Francesco and Wang, Yingheng and Shridhar, Kumar and Overholt, Kalon J. and Briia, Glib and Nguyen, Hieu and David and Bartomeu, Soler and Pang, Tony CY and Wecker, Adam and Xiong, Yifan and Li, Fanfei and Huber, Lukas S. and Jaeger, Joshua and Maddalena, Romano De and L{\`u}, Xing Han and Zhang, Yuhui and Beger, Claas and Kon, Patrick Tser Jern and Li, Sean and Sanker, Vivek and Yin, Ming and Liang, Yihao and Zhang, Xinlu and Agrawal, Ankit and Yifei, Li S. and Zhang, Zechen and Cai, Mu and Sonmez, Yasin and Cozianu, Costin and Li, Changhao and Slen, Alex and Yu, Shoubin and Park, Hyun Kyu and Sarti, Gabriele and Bria{\'n}ski, Marcin and Stolfo, Alessandro and Nguyen, Truong An and Zhang, Mike and Perlitz, Yotam and {Hernandez-Orallo}, Jose and Li, Runjia and Shabani, Amin and {Juefei-Xu}, Felix and Dhingra, Shikhar and Zohar, Orr and Nguyen, My Chiffon and Pondaven, Alexander and Yilmaz, Abdurrahim and Zhao, Xuandong and Jin, Chuanyang and Jiang, Muyan and Todoran, Stefan and Han, Xinyao and Kreuer, Jules and Rabern, Brian and Plassart, Anna and Maggetti, Martino and Yap, Luther and Geirhos, Robert and Kean, Jonathon and Wang, Dingsu and Mollaei, Sina and Sun, Chenkai and Yin, Yifan and Wang, Shiqi and Li, Rui and Chang, Yaowen and Wei, Anjiang and Bizeul, Alice and Wang, Xiaohan and Arrais, Alexandre Oliveira and Mukherjee, Kushin and {Chamorro-Padial}, Jorge and Liu, Jiachen and Qu, Xingyu and Guan, Junyi and Bouyamourn, Adam and Wu, Shuyu and Plomecka, Martyna and Chen, Junda and Tang, Mengze and Deng, Jiaqi and Subramanian, Shreyas and Xi, Haocheng and Chen, Haoxuan and Zhang, Weizhi and Ren, Yinuo and Tu, Haoqin and Kim, Sejong and Chen, Yushun and Marjanovi{\'c}, Sara Vera and Ha, Junwoo and Luczyna, Grzegorz and Ma, Jeff J. and Shen, Zewen and Song, Dawn and Zhang, Cedegao E. and Wang, Zhun and Gendron, Ga{\"e}l and Xiao, Yunze and Smucker, Leo and Weng, Erica and Lee, Kwok Hao and Ye, Zhe and Ermon, Stefano and {Lopez-Miguel}, Ignacio D. and Knights, Theo and Gitter, Anthony and Park, Namkyu and Wei, Boyi and Chen, Hongzheng and Pai, Kunal and Elkhanany, Ahmed and Lin, Han and Siedler, Philipp D. and Fang, Jichao and Mishra, Ritwik and {Zsolnai-Feh{\'e}r}, K{\'a}roly and Jiang, Xilin and Khan, Shadab and Yuan, Jun and Jain, Rishab Kumar and Lin, Xi and Peterson, Mike and Wang, Zhe and Malusare, Aditya and Tang, Maosen and Gupta, Isha and Fosin, Ivan and Kang, Timothy and Dworakowska, Barbara and Matsumoto, Kazuki and Zheng, Guangyao and Sewuster, Gerben and Villanueva, Jorge Pretel and Rannev, Ivan and Chernyavsky, Igor and Chen, Jiale and Banik, Deepayan and Racz, Ben and Dong, Wenchao and Wang, Jianxin and Bashmal, Laila and Gon{\c c}alves, Duarte V. and Hu, Wei and Bar, Kaushik and Bohdal, Ondrej and Patlan, Atharv Singh and Dhuliawala, Shehzaad and Geirhos, Caroline and Wist, Julien and Kansal, Yuval and Chen, Bingsen and Tire, Kutay and Y{\"u}cel, Atak Talay and Christof, Brandon and Singla, Veerupaksh and Song, Zijian and Chen, Sanxing and Ge, Jiaxin and Ponkshe, Kaustubh and Park, Isaac and Shi, Tianneng and Ma, Martin Q. and Mak, Joshua and Lai, Sherwin and Moulin, Antoine and Cheng, Zhuo and Zhu, Zhanda and Zhang, Ziyi and Patil, Vaidehi and Jha, Ketan and Men, Qiutong and Wu, Jiaxuan and Zhang, Tianchi and Vieira, Bruno Hebling and Aji, Alham Fikri and Chung, Jae-Won and Mahfoud, Mohammed and Hoang, Ha Thi and Sperzel, Marc and Hao, Wei and Meding, Kristof and Xu, Sihan and Kostakos, Vassilis and Manini, Davide and Liu, Yueying and Toukmaji, Christopher and Paek, Jay and Yu, Eunmi and Demircali, Arif Engin and Sun, Zhiyi and Dewerpe, Ivan and Qin, Hongsen and Pflugfelder, Roman and Bailey, James and Morris, Johnathan and Heilala, Ville and Rosset, Sybille and Yu, Zishun and Chen, Peter E. and Yeo, Woongyeong and Jain, Eeshaan and Yang, Ryan and Chigurupati, Sreekar and Chernyavsky, Julia and Reddy, Sai Prajwal and Venugopalan, Subhashini and Batra, Hunar and Park, Core Francisco and Tran, Hieu and Maximiano, Guilherme and Zhang, Genghan and Liang, Yizhuo and Shiyu, Hu and Xu, Rongwu and Pan, Rui and Suresh, Siddharth and Liu, Ziqi and Gulati, Samaksh and Zhang, Songyang and Turchin, Peter and Bartlett, Christopher W. and Scotese, Christopher R. and Cao, Phuong M. and Wu, Ben and Karwowski, Jacek and Scaramuzza, Davide and Nattanmai, Aakaash and McKellips, Gordon and Cheraku, Anish and Suhail, Asim and Luo, Ethan and Deng, Marvin and Luo, Jason and Zhang, Ashley and Jindel, Kavin and Paek, Jay and Halevy, Kasper and Baranov, Allen and Liu, Michael and Avadhanam, Advaith and Zhang, David and Cheng, Vincent and Ma, Brad and Fu, Evan and Do, Liam and Lass, Joshua and Yang, Hubert and Sunkari, Surya and Bharath, Vishruth and Ai, Violet and Leung, James and Agrawal, Rishit and Zhou, Alan and Chen, Kevin and Kalpathi, Tejas and Xu, Ziqi and Wang, Gavin and Xiao, Tyler and Maung, Erik and Lee, Sam and Yang, Ryan and Yue, Roy and Zhao, Ben and Yoon, Julia and Sun, Sunny and Singh, Aryan and Luo, Ethan and Peng, Clark and Osbey, Tyler and Wang, Taozhi and Echeazu, Daryl and Yang, Hubert and Wu, Timothy and Patel, Spandan and Kulkarni, Vidhi and Sundarapandiyan, Vijaykaarti and Zhang, Ashley and Le, Andrew and Nasim, Zafir and Yalam, Srikar and Kasamsetty, Ritesh and Samal, Soham and Yang, Hubert and Sun, David and Shah, Nihar and Saha, Abhijeet and Zhang, Alex and Nguyen, Leon and Nagumalli, Laasya and Wang, Kaixin and Zhou, Alan and Wu, Aidan and Luo, Jason and Telluri, Anwith and Yue, Summer and Wang, Alexandr and Hendrycks, Dan},
  year = 2025,
  month = sep,
  number = {arXiv:2501.14249},
  eprint = {2501.14249},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.14249},
  urldate = {2025-09-29},
  abstract = {Benchmarks are important tools for tracking the rapid advancements in large language model (LLM) capabilities. However, benchmarks are not keeping pace in difficulty: LLMs now achieve over 90\textbackslash\% accuracy on popular benchmarks like MMLU, limiting informed measurement of state-of-the-art LLM capabilities. In response, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at the frontier of human knowledge, designed to be the final closed-ended academic benchmark of its kind with broad subject coverage. HLE consists of 2,500 questions across dozens of subjects, including mathematics, humanities, and the natural sciences. HLE is developed globally by subject-matter experts and consists of multiple-choice and short-answer questions suitable for automated grading. Each question has a known solution that is unambiguous and easily verifiable, but cannot be quickly answered via internet retrieval. State-of-the-art LLMs demonstrate low accuracy and calibration on HLE, highlighting a significant gap between current LLM capabilities and the expert human frontier on closed-ended academic questions. To inform research and policymaking upon a clear understanding of model capabilities, we publicly release HLE at https://lastexam.ai.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/yiyuanli/Zotero/storage/3FKMR8GQ/Phan et al. - 2025 - Humanity's Last Exam.pdf;/Users/yiyuanli/Zotero/storage/EXGCC3GH/2501.html}
}

@article{prakashFULLYUNSUPERVISEDDIVERSITY2021,
  title = {{{FULLY UNSUPERVISED DIVERSITY DENOISING WITH CONVOLUTIONAL VARIATIONAL AUTOENCODERS}}},
  author = {Prakash, Mangal and Krull, Alexander and Jug, Florian},
  year = 2021,
  abstract = {Deep Learning based methods have emerged as the indisputable leaders for virtually all image restoration tasks. Especially in the domain of microscopy images, various content-aware image restoration (CARE) approaches are now used to improve the interpretability of acquired data. Naturally, there are limitations to what can be restored in corrupted images, and like for all inverse problems, many potential solutions exist, and one of them must be chosen. Here, we propose DIVNOISING, a denoising approach based on fully convolutional variational autoencoders (VAEs), overcoming the problem of having to choose a single solution by predicting a whole distribution of denoised images. First we introduce a principled way of formulating the unsupervised denoising problem within the VAE framework by explicitly incorporating imaging noise models into the decoder. Our approach is fully unsupervised, only requiring noisy images and a suitable description of the imaging noise distribution. We show that such a noise model can either be measured, bootstrapped from noisy data, or co-learned during training. If desired, consensus predictions can be inferred from a set of DIVNOISING predictions, leading to competitive results with other unsupervised methods and, on occasion, even with the supervised state-of-the-art. DIVNOISING samples from the posterior enable a plethora of useful applications. We are piq showing denoising results for 13 datasets, piiq discussing how optical character recognition (OCR) applications can benefit from diverse predictions, and are piiiq demonstrating how instance cell segmentation improves when using diverse DIVNOISING predictions.},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/68WK3WAH/Prakash et al. - 2021 - FULLY UNSUPERVISED DIVERSITY DENOISING WITH CONVOLUTIONAL VARIATIONAL AUTOENCODERS.pdf}
}

@article{przeworskiPoliticalRegimesEconomic1993,
  title = {Political {{Regimes}} and {{Economic Growth}}},
  author = {Przeworski, Adam and Limongi, Fernando},
  year = 1993,
  journal = {The Journal of Economic Perspectives},
  volume = {7},
  number = {3},
  eprint = {2138442},
  eprinttype = {jstor},
  pages = {51--69},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/Y8DD3TJ8/PrzeworskiLimongi_1993.pdf}
}

@inproceedings{qiADELIEAligningLarge2024,
  title = {{{ADELIE}}: {{Aligning Large Language Models}} on {{Information Extraction}}},
  shorttitle = {{{ADELIE}}},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Qi, Yunjia and Peng, Hao and Wang, Xiaozhi and Xu, Bin and Hou, Lei and Li, Juanzi},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = 2024,
  month = nov,
  pages = {7371--7387},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.419},
  urldate = {2024-12-16},
  file = {/Users/yiyuanli/Zotero/storage/NFVPJAT6/Qi et al. - 2024 - ADELIE Aligning Large Language Models on Informat.pdf}
}

@misc{qiADELIEAligningLarge2024a,
  title = {{{ADELIE}}: {{Aligning Large Language Models}} on {{Information Extraction}}},
  shorttitle = {{{ADELIE}}},
  author = {Qi, Yunjia and Peng, Hao and Wang, Xiaozhi and Xu, Bin and Hou, Lei and Li, Juanzi},
  year = 2024,
  month = oct,
  number = {arXiv:2405.05008},
  eprint = {2405.05008},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.05008},
  urldate = {2024-12-16},
  abstract = {Large language models (LLMs) usually fall short on information extraction (IE) tasks and struggle to follow the complex instructions of IE tasks. This primarily arises from LLMs not being aligned with humans, as mainstream alignment datasets typically do not include IE data. In this paper, we introduce ADELIE (Aligning large language moDELs on Information Extraction), an aligned LLM that effectively solves various IE tasks, including closed IE, open IE, and on-demand IE. We first collect and construct a high-quality alignment corpus IEInstruct for IE. Then we train ADELIE\_SFT using instruction tuning on IEInstruct. We further train ADELIE\_SFT with direct preference optimization (DPO) objective, resulting in ADELIE\_DPO. Extensive experiments on various held-out IE datasets demonstrate that our models (ADELIE\_SFT and ADELIE\_DPO) achieve state-of-the-art (SoTA) performance among open-source models. We further explore the general capabilities of ADELIE, and experimental results reveal that their general capabilities do not exhibit a noticeable decline. We will release the code, data, and models to facilitate further research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/yiyuanli/Zotero/storage/MJBD7C5P/Qi et al. - 2024 - ADELIE Aligning Large Language Models on Informat.pdf;/Users/yiyuanli/Zotero/storage/8JP8TVYG/2405.html}
}

@article{radfordLanguageModelsAre,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/XGM2LJ32/Radford et al. - Language Models are Unsupervised Multitask Learners.pdf}
}

@misc{rahmanText2VisChallengingDiverse2025,
  title = {{{Text2Vis}}: {{A Challenging}} and {{Diverse Benchmark}} for {{Generating Multimodal Visualizations}} from {{Text}}},
  shorttitle = {{{Text2Vis}}},
  author = {Rahman, Mizanur and Laskar, Md Tahmid Rahman and Joty, Shafiq and Hoque, Enamul},
  year = 2025,
  month = jul,
  number = {arXiv:2507.19969},
  eprint = {2507.19969},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2507.19969},
  urldate = {2025-08-30},
  abstract = {Automated data visualization plays a crucial role in simplifying data interpretation, enhancing decision-making, and improving efficiency. While large language models (LLMs) have shown promise in generating visualizations from natural language, the absence of comprehensive benchmarks limits the rigorous evaluation of their capabilities. We introduce Text2Vis, a benchmark designed to assess textto-visualization models, covering 20+ chart types and diverse data science queries, including trend analysis, correlation, outlier detection, and predictive analytics. It comprises 1,985 samples, each with a data table, natural language query, short answer, visualization code, and annotated charts. The queries involve complex reasoning, conversational turns, and dynamic data retrieval. We benchmark 11 open-source and closed-source models, revealing significant performance gaps, highlighting key challenges, and offering insights for future advancements. To close this gap, we propose the first cross-modal actor-critic agentic framework that jointly refines the textual answer and visualization code, increasing GPT-4o's pass rate from 26\% to 42\% over the direct approach and improving chart quality. We also introduce an automated LLM-based evaluation framework that enables scalable assessment across thousands of samples without human annotation, measuring answer correctness, code execution success, visualization readability, and chart accuracy. We release Text2Vis at https: //github.com/vis-nlp/Text2Vis.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/yiyuanli/Zotero/storage/MGFQMY6F/Rahman et al. - 2025 - Text2Vis A Challenging and Diverse Benchmark for Generating Multimodal Visualizations from Text.pdf}
}

@misc{raviSAM2Segment2024,
  title = {{{SAM}} 2: {{Segment Anything}} in {{Images}} and {{Videos}}},
  shorttitle = {{{SAM}} 2},
  author = {Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and R{\"a}dle, Roman and Rolland, Chloe and Gustafson, Laura and Mintun, Eric and Pan, Junting and Alwala, Kalyan Vasudev and Carion, Nicolas and Wu, Chao-Yuan and Girshick, Ross and Doll{\'a}r, Piotr and Feichtenhofer, Christoph},
  year = 2024,
  month = oct,
  number = {arXiv:2408.00714},
  eprint = {2408.00714},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.00714},
  urldate = {2025-09-28},
  abstract = {We present Segment Anything Model 2 (SAM 2), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing our main model, dataset, as well as code for model training and our demo.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/yiyuanli/Zotero/storage/56C42VWJ/Ravi et al. - 2024 - SAM 2 Segment Anything in Images and Videos.pdf}
}

@misc{reinGPQAGraduateLevelGoogleProof2023,
  title = {{{GPQA}}: {{A Graduate-Level Google-Proof Q}}\&{{A Benchmark}}},
  shorttitle = {{{GPQA}}},
  author = {Rein, David and Hou, Betty Li and Stickland, Asa Cooper and Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien and Michael, Julian and Bowman, Samuel R.},
  year = 2023,
  month = nov,
  number = {arXiv:2311.12022},
  eprint = {2311.12022},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.12022},
  urldate = {2025-09-29},
  abstract = {We present GPQA, a challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. We ensure that the questions are high-quality and extremely difficult: experts who have or are pursuing PhDs in the corresponding domains reach 65\% accuracy (74\% when discounting clear mistakes the experts identified in retrospect), while highly skilled non-expert validators only reach 34\% accuracy, despite spending on average over 30 minutes with unrestricted access to the web (i.e., the questions are "Google-proof"). The questions are also difficult for state-of-the-art AI systems, with our strongest GPT-4 based baseline achieving 39\% accuracy. If we are to use future AI systems to help us answer very hard questions, for example, when developing new scientific knowledge, we need to develop scalable oversight methods that enable humans to supervise their outputs, which may be difficult even if the supervisors are themselves skilled and knowledgeable. The difficulty of GPQA both for skilled non-experts and frontier AI systems should enable realistic scalable oversight experiments, which we hope can help devise ways for human experts to reliably get truthful information from AI systems that surpass human capabilities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/yiyuanli/Zotero/storage/5H6NLGDG/Rein et al. - 2023 - GPQA A Graduate-Level Google-Proof Q&A Benchmark.pdf;/Users/yiyuanli/Zotero/storage/LZYN65X3/2311.html}
}

@misc{renNextTokenNextXPrediction2025,
  title = {Beyond {{Next-Token}}: {{Next-X Prediction}} for {{Autoregressive Visual Generation}}},
  shorttitle = {Beyond {{Next-Token}}},
  author = {Ren, Sucheng and Yu, Qihang and He, Ju and Shen, Xiaohui and Yuille, Alan and Chen, Liang-Chieh},
  year = 2025,
  month = mar,
  number = {arXiv:2502.20388},
  eprint = {2502.20388},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.20388},
  urldate = {2025-10-06},
  abstract = {Autoregressive (AR) modeling, known for its next-token prediction paradigm, underpins state-of-the-art language and visual generative models. Traditionally, a ``token'' is treated as the smallest prediction unit, often a discrete symbol in language or a quantized patch in vision. However, the optimal token definition for 2D image structures remains an open question. Moreover, AR models suffer from exposure bias, where teacher forcing during training leads to error accumulation at inference. In this paper, we propose xAR, a generalized AR framework that extends the notion of a token to an entity X, which can represent an individual patch token, a cell (a \$k\textbackslash times k\$ grouping of neighboring patches), a subsample (a non-local grouping of distant patches), a scale (coarse-to-fine resolution), or even a whole image. Additionally, we reformulate discrete token classification as continuous entity regression, leveraging flow-matching methods at each AR step. This approach conditions training on noisy entities instead of ground truth tokens, leading to Noisy Context Learning, which effectively alleviates exposure bias. As a result, xAR offers two key advantages: (1) it enables flexible prediction units that capture different contextual granularity and spatial structures, and (2) it mitigates exposure bias by avoiding reliance on teacher forcing. On ImageNet-256 generation benchmark, our base model, xAR-B (172M), outperforms DiT-XL/SiT-XL (675M) while achieving 20\$\textbackslash times\$ faster inference. Meanwhile, xAR-H sets a new state-of-the-art with an FID of 1.24, running 2.2\$\textbackslash times\$ faster than the previous best-performing model without relying on vision foundation modules (e.g., DINOv2) or advanced guidance interval sampling.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/yiyuanli/Zotero/storage/2ACFFTU5/Ren et al. - 2025 - Beyond Next-Token Next-X Prediction for Autoregressive Visual Generation.pdf}
}

@inproceedings{renzeEffectSamplingTemperature2024,
  title = {The {{Effect}} of {{Sampling Temperature}} on {{Problem Solving}} in {{Large Language Models}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2024},
  author = {Renze, Matthew and Guven, Erhan},
  year = 2024,
  eprint = {2402.05201},
  primaryclass = {cs},
  pages = {7346--7356},
  doi = {10.18653/v1/2024.findings-emnlp.432},
  urldate = {2025-10-06},
  abstract = {In this research study, we empirically investigate the effect of sampling temperature on the performance of Large Language Models (LLMs) on various problem-solving tasks. We created a multiple-choice question-and-answer (MCQA) exam by randomly sampling problems from standard LLM benchmarks. Then, we used four popular LLMs with five prompt-engineering techniques to solve the MCQA problems while increasing the sampling temperature from 0.0 to 1.0. Despite anecdotal reports to the contrary, our empirical results indicate that changes in temperature in the range 0.0 to 1.0 do not have a statistically significant impact on LLM performance for problem-solving tasks. In addition, these results appear to hold regardless of the LLM, the prompt-engineering technique, or the problem domain. All code, data, and supplemental materials are available on GitHub at: https://github.com/matthewrenze/ jhu-llm-temperature.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/yiyuanli/Zotero/storage/FAMNYJ3Y/Renze and Guven - 2024 - The Effect of Sampling Temperature on Problem Solving in Large Language Models.pdf}
}

@article{sahaiDiscreteMathematicsProbability2013,
  title = {Discrete {{Mathematics}} and {{Probability Theory}}},
  author = {Sahai, Anant},
  year = 2013,
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/RBD6TN2V/Sahai - 2013 - Discrete Mathematics and Probability Theory.pdf}
}

@inproceedings{satyanarayanDeclarativeInteractionDesign2014,
  title = {Declarative Interaction Design for Data Visualization},
  booktitle = {Proceedings of the 27th Annual {{ACM}} Symposium on {{User}} Interface Software and Technology},
  author = {Satyanarayan, Arvind and Wongsuphasawat, Kanit and Heer, Jeffrey},
  year = 2014,
  month = oct,
  pages = {669--678},
  publisher = {ACM},
  address = {Honolulu Hawaii USA},
  doi = {10.1145/2642918.2647360},
  urldate = {2025-09-28},
  abstract = {Declarative visualization grammars can accelerate development, facilitate retargeting across platforms, and allow language-level optimizations. However, existing declarative visualization languages are primarily concerned with visual encoding, and rely on imperative event handlers for interactive behaviors. In response, we introduce a model of declarative interaction design for data visualizations. Adopting methods from reactive programming, we model low-level events as composable data streams from which we form higher-level semantic signals. Signals feed predicates and scale inversions, which allow us to generalize interactive selections at the level of item geometry (pixels) into interactive queries over the data domain. Production rules then use these queries to manipulate the visualization's appearance. To facilitate reuse and sharing, these constructs can be encapsulated as named interactors: standalone, purely declarative specifications of interaction techniques. We assess our model's feasibility and expressivity by instantiating it with extensions to the Vega visualization grammar. Through a diverse range of examples, we demonstrate coverage over an established taxonomy of visualization interaction techniques.},
  isbn = {978-1-4503-3069-5},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/AZI4TDQR/Satyanarayan et al. - 2014 - Declarative interaction design for data visualization.pdf}
}

@misc{shanny-csikAddingBlogYour2022,
  title = {Adding a Blog to Your Existing {{Quarto}} Website -- {{Sam Shanny-Csik}}},
  author = {{Shanny-Csik}, Samantha},
  year = 2022,
  month = oct,
  urldate = {2025-10-27},
  abstract = {Got a Quarto website, but no blog? We can fix that!},
  howpublished = {https://samanthacsik.github.io/posts/2022-10-24-quarto-blogs/},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/3GLFEGS6/2022-10-24-quarto-blogs.html}
}

@misc{shanny-csikCreatingYourPersonal2022,
  title = {Creating Your Personal Website Using {{Quarto}}},
  author = {{Shanny-Csik}, Sam},
  year = 2022,
  month = aug,
  urldate = {2025-10-27},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/AHUBL43P/creating-quarto-websites.html}
}

@misc{sharmaVisionCheckupLanguage2024,
  title = {A {{Vision Check-up}} for {{Language Models}}},
  author = {Sharma, Pratyusha and Shaham, Tamar Rott and Baradad, Manel and Fu, Stephanie and {Rodriguez-Munoz}, Adrian and Duggal, Shivam and Isola, Phillip and Torralba, Antonio},
  year = 2024,
  month = jan,
  number = {arXiv:2401.01862},
  eprint = {2401.01862},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.01862},
  urldate = {2025-09-02},
  abstract = {What does learning to model relationships between strings teach Large Language Models (LLMs) about the visual world? We systematically evaluate LLMs' abilities to generate and recognize an assortment of visual concepts of increasing complexity and then demonstrate how a preliminary visual representation learning system can be trained using models of text. As language models lack the ability to consume or output visual information as pixels, we use code to represent images in our study. Although LLM-generated images do not look like natural images, results on image generation and the ability of models to correct these generated images indicate that precise modeling of strings can teach language models about numerous aspects of the visual world. Furthermore, experiments on self-supervised visual representation learning, utilizing images generated with text models, highlight the potential to train vision models capable of making semantic assessments of natural images using just LLMs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/yiyuanli/Zotero/storage/SXHKLATX/Sharma et al. - 2024 - A Vision Check-up for Language Models.pdf}
}

@misc{shethNeurosymbolicAIWhy2023,
  title = {Neurosymbolic {{AI}} -- {{Why}}, {{What}}, and {{How}}},
  author = {Sheth, Amit and Roy, Kaushik and Gaur, Manas},
  year = 2023,
  month = may,
  number = {arXiv:2305.00813},
  eprint = {2305.00813},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.00813},
  urldate = {2025-09-02},
  abstract = {Humans interact with the environment using a combination of perception - transforming sensory inputs from their environment into symbols, and cognition - mapping symbols to knowledge about the environment for supporting abstraction, reasoning by analogy, and long-term planning. Human perception-inspired machine perception, in the context of AI, refers to large-scale pattern recognition from raw data using neural networks trained using self-supervised learning objectives such as next-word prediction or object recognition. On the other hand, machine cognition encompasses more complex computations, such as using knowledge of the environment to guide reasoning, analogy, and long-term planning. Humans can also control and explain their cognitive functions. This seems to require the retention of symbolic mappings from perception outputs to knowledge about their environment. For example, humans can follow and explain the guidelines and safety constraints driving their decision-making in safety-critical applications such as healthcare, criminal justice, and autonomous driving. While datadriven neural network-based AI algorithms effectively model machine perception, symbolic knowledge-based AI is better suited for modeling machine cognition. This is because symbolic knowledge structures support explicit representations of mappings from perception outputs to the knowledge, enabling traceability and auditing of the AI system's decisions. Such audit trails are useful for enforcing application aspects of safety, such as regulatory compliance and explainability, through tracking the AI system's inputs, outputs, and intermediate steps. This first article in the Neurosymbolic AI department introduces and provides an overview of the rapidly emerging paradigm of Neurosymbolic AI, combining neural networks and knowledge-guided symbolic approaches to create more capable and flexible AI systems. These systems have immense potential to advance both algorithm-level (e.g., abstraction, analogy, reasoning) and application-level (e.g., explainable and safety-constrained decision-making) capabilities of AI systems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/yiyuanli/Zotero/storage/YCUVBQMI/Sheth et al. - 2023 - Neurosymbolic AI -- Why, What, and How.pdf}
}

@article{stasuikEvaluatingLLMPerformance,
  title = {Evaluating {{LLM Performance}} in {{Essay Assessment}}: {{A Comparative Analysis}} of {{AI Grading}} and {{Feedback Systems}} for {{University English Courses}}},
  author = {Stasuik, Noah Carter},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/M6BJEG7B/Stasuik - Evaluating LLM Performance in Essay Assessment A Comparative Analysis of AI Grading and Feedback Sy.pdf}
}

@article{stringerCellpose3OneclickImage2025,
  title = {Cellpose3: One-Click Image Restoration for Improved Cellular Segmentation},
  shorttitle = {Cellpose3},
  author = {Stringer, Carsen and Pachitariu, Marius},
  year = 2025,
  month = mar,
  journal = {Nature Methods},
  volume = {22},
  number = {3},
  pages = {592--599},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-025-02595-5},
  urldate = {2025-10-08},
  abstract = {Generalist methods for cellular segmentation have good out-of-the-box performance on a variety of image types; however, existing methods struggle for images that are degraded by noise, blurring or undersampling, all of which are common in microscopy. We focused the development of Cellpose3 on addressing these cases and here we demonstrate substantial out-of-the-box gains in segmentation and image quality for noisy, blurry and undersampled images. Unlike previous approaches that train models to restore pixel values, we trained Cellpose3 to output images that are well segmented by a generalist segmentation model, while maintaining perceptual similarity to the target images. Furthermore, we trained the restoration models on a large, varied collection of datasets, thus ensuring good generalization to user images. We provide these tools as `one-click' buttons inside the graphical interface of Cellpose as well as in the Cellpose API.},
  copyright = {2025 The Author(s)},
  langid = {english},
  keywords = {Image processing,Machine learning},
  file = {/Users/yiyuanli/Zotero/storage/KNUDBFNM/Stringer and Pachitariu - 2025 - Cellpose3 one-click image restoration for improved cellular segmentation.pdf}
}

@article{sunGeneralizedMcNemarsTest2008,
  title = {Generalized {{McNemar}}'s {{Test}} for {{Homogeneity}} of the {{Marginal Distributions}}},
  author = {Sun, Xuezheng and Yang, Zhao},
  year = 2008,
  abstract = {In the matched-pairs data, McNemar's test (McNemar, 1947) can be applied only to the case in which there are two possible categories for the outcome. In practice, however, it is possible that the outcomes are classified into multiple categories. Under this situation, the test statistic proposed by Stuart (1955) and Maxwell (1970) is useful, it is actually the generalization of the McNemar's test, commonly referred to as generalized McNemar's or Stuart-Maxwell test. There is no public available SAS program to calculate this statistic, the author has developed a SAS macro (the code is detailed in appendix) to perform this test and briefly describes how to use the macro. Examples using the developed SAS macro are also included.},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/CR33ZWPI/Sun and Yang - 2008 - Generalized McNemar’s Test for Homogeneity of the Marginal Distributions.pdf}
}

@misc{sunVRPSAMSAMVisual2024,
  title = {{{VRP-SAM}}: {{SAM}} with {{Visual Reference Prompt}}},
  shorttitle = {{{VRP-SAM}}},
  author = {Sun, Yanpeng and Chen, Jiahui and Zhang, Shan and Zhang, Xinyu and Chen, Qiang and Zhang, Gang and Ding, Errui and Wang, Jingdong and Li, Zechao},
  year = 2024,
  month = mar,
  number = {arXiv:2402.17726},
  eprint = {2402.17726},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.17726},
  urldate = {2025-09-28},
  abstract = {In this paper, we propose a novel Visual Reference Prompt (VRP) encoder that empowers the Segment Anything Model (SAM) to utilize annotated reference images as prompts for segmentation, creating the VRP-SAM model. In essence, VRP-SAM can utilize annotated reference images to comprehend specific objects and perform segmentation of specific objects in target image. It is note that the VRP encoder can support a variety of annotation formats for reference images, including \textbackslash textbf\textbraceleft point\textbraceright, \textbackslash textbf\textbraceleft box\textbraceright, \textbackslash textbf\textbraceleft scribble\textbraceright, and \textbackslash textbf\textbraceleft mask\textbraceright. VRP-SAM achieves a breakthrough within the SAM framework by extending its versatility and applicability while preserving SAM's inherent strengths, thus enhancing user-friendliness. To enhance the generalization ability of VRP-SAM, the VRP encoder adopts a meta-learning strategy. To validate the effectiveness of VRP-SAM, we conducted extensive empirical studies on the Pascal and COCO datasets. Remarkably, VRP-SAM achieved state-of-the-art performance in visual reference segmentation with minimal learnable parameters. Furthermore, VRP-SAM demonstrates strong generalization capabilities, allowing it to perform segmentation of unseen objects and enabling cross-domain segmentation. The source code and models will be available at \textbackslash url\textbraceleft https://github.com/syp2ysy/VRP-SAM\textbraceright},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/yiyuanli/Zotero/storage/GWAVWJDQ/Sun et al. - 2024 - VRP-SAM SAM with Visual Reference Prompt.pdf}
}

@misc{SupplyChainGreenhouse2024,
  title = {Supply {{Chain Greenhouse Gas Emission Factors}} v1.3 by {{NAICS-6}}},
  year = 2024,
  month = jul,
  publisher = {{U.S. EPA Office of Research and Development (ORD)}},
  urldate = {2025-11-24},
  abstract = {The datasets comprise greenhouse gas (GHG) emission factors (Factors) for 1,016 U.S. commodities as defined by the 2017 version of the North American Industry Classification System (NAICS). The Factors are based on GHG data for 2022. Factors are given for all NAICS-defined commodities at the 6-digit level except for electricity, government, and households. Each record consists of three factor types as in the previous releases: Supply Chain Emissions without Margins (SEF), Margins of Supply Chain Emissions (MEF), and Supply Chain Emissions with Margins (SEF+MEF). One set of Factors provides kg carbon dioxide equivalents (CO2e) per 2022 U.S. dollar (USD) for all GHGs combined using 100-yr global warming potentials from IPCC 5th report (AR5) to calculate the equivalents. In this dataset there is one SEF, MEF and SEF+MEF per commodity. The other dataset of Factors provides kg of each unique GHG emitted per 2022 dollar per commodity without the CO2e calculation. The dollar in the denominator of all factors uses purchaser prices. See the supporting file 'Aboutv1.3SupplyChainGHGEmissionFactors.docx' for complete documentation of this dataset.},
  langid = {english},
  keywords = {climate-change,ghg-reporting,industry-ghgs,scope-3,useeio}
}

@misc{thesglangteamDeterministicInferenceSGLang,
  title = {Towards {{Deterministic Inference}} in {{SGLang}} and {{Reproducible RL Training}} \textbar{} {{LMSYS Org}}},
  author = {The SGLang Team},
  urldate = {2025-10-06},
  abstract = {{$<$}p{$><$}strong{$>$}TL;DR{$<$}/strong{$>$}: This post shares our efforts to enable deterministic inference in SGLang and our collaboration with},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/4R3HTZ8G/2025-09-22-sglang-deterministic.html}
}

@misc{TopWebsitesWorld,
  title = {Top {{Websites}} in the {{World}} - {{September}} 2025 {{Most Visited}} \& {{Popular Rankings}}},
  journal = {Semrush},
  urldate = {2025-11-03},
  abstract = {Get the latest September 2025 website rankings in the World with Semrush: traffic, conversion, and engagement insights.},
  howpublished = {https://www.semrush.com/website/top/},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/PBWFKBJL/top.html}
}

@misc{tranPreservingGeneralizationLanguage2024,
  title = {Preserving {{Generalization}} of {{Language}} Models in {{Few-shot Continual Relation Extraction}}},
  author = {Tran, Quyen and Thanh, Nguyen Xuan and Anh, Nguyen Hoang and Hai, Nam Le and Le, Trung and Ngo, Linh Van and Nguyen, Thien Huu},
  year = 2024,
  month = oct,
  number = {arXiv:2410.00334},
  eprint = {2410.00334},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.00334},
  urldate = {2024-12-16},
  abstract = {Few-shot Continual Relations Extraction (FCRE) is an emerging and dynamic area of study where models can sequentially integrate knowledge from new relations with limited labeled data while circumventing catastrophic forgetting and preserving prior knowledge from pre-trained backbones. In this work, we introduce a novel method that leverages often-discarded language model heads. By employing these components via a mutual information maximization strategy, our approach helps maintain prior knowledge from the pre-trained backbone and strategically aligns the primary classification head, thereby enhancing model performance. Furthermore, we explore the potential of Large Language Models (LLMs), renowned for their wealth of knowledge, in addressing FCRE challenges. Our comprehensive experimental results underscore the efficacy of the proposed method and offer valuable insights for future work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/yiyuanli/Zotero/storage/NK2K462U/Tran et al. - 2024 - Preserving Generalization of Language models in Fe.pdf;/Users/yiyuanli/Zotero/storage/GBRNXK4V/2410.html}
}

@misc{vaswaniAttentionAllYou2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = 2023,
  month = aug,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.03762},
  urldate = {2025-12-18},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/yiyuanli/Zotero/storage/6V8XIRXJ/Vaswani et al. - 2023 - Attention Is All You Need.pdf;/Users/yiyuanli/Zotero/storage/ZXVJXGYM/1706.html}
}

@misc{VisionCheckupLanguage,
  title = {A {{Vision Check-up}} for {{Language Models}}},
  urldate = {2025-09-02},
  howpublished = {https://arxiv.org/html/2401.01862v1},
  file = {/Users/yiyuanli/Zotero/storage/VGW5HIP2/2401.html}
}

@inproceedings{wangBioRFXRefiningBiomedical2024,
  title = {Bio-{{RFX}}: {{Refining Biomedical Extraction}} via {{Advanced Relation Classification}} and {{Structural Constraints}}},
  shorttitle = {Bio-{{RFX}}},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Wang, Minjia and Liu, Fangzhou and Li, Xiuxing and Dong, Bowen and Li, Zhenyu and Pan, Tengyu and Wang, Jianyong},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = 2024,
  month = nov,
  pages = {10524--10539},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.588},
  urldate = {2024-12-16},
  abstract = {The ever-growing biomedical publications magnify the challenge of extracting structured data from unstructured texts. This task involves two components: biomedical entity identification (Named Entity Recognition, NER) and their interrelation determination (Relation Extraction, RE). However, existing methods often neglect unique features of the biomedical literature, such as ambiguous entities, nested proper nouns, and overlapping relation triplets, and underutilize prior knowledge, leading to an intolerable performance decline in the biomedical domain, especially with limited annotated training data. In this paper, we propose the Biomedical Relation-First eXtraction (Bio-RFX) model by leveraging sentence-level relation classification before entity extraction to tackle entity ambiguity. Moreover, we exploit structural constraints between entities and relations to guide the model's hypothesis space, enhancing extraction performance across different training scenarios. Comprehensive experimental results on biomedical datasets show that Bio-RFX achieves significant improvements on both NER and RE tasks. Even under the low-resource training scenarios, it outperforms all baselines in NER and has highly competitive performance compared to the state-of-the-art fine-tuned baselines in RE.},
  file = {/Users/yiyuanli/Zotero/storage/V4X54FTB/Wang et al. - 2024 - Bio-RFX Refining Biomedical Extraction via Advanc.pdf}
}

@misc{wangFDABenchBenchmarkData2025,
  title = {{{FDABench}}: {{A Benchmark}} for {{Data Agents}} on {{Analytical Queries}} over {{Heterogeneous Data}}},
  shorttitle = {{{FDABench}}},
  author = {Wang, Ziting and Zhang, Shize and Yuan, Haitao and Zhu, Jinwei and Li, Shifu and Dong, Wei and Cong, Gao},
  year = 2025,
  month = sep,
  number = {arXiv:2509.02473},
  eprint = {2509.02473},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2509.02473},
  urldate = {2025-09-12},
  abstract = {The growing demand for data-driven decision-making has created an urgent need for data agents that can integrate structured and unstructured data for analysis. While data agents show promise for enabling users to perform complex analytics tasks, this field still suffers from three critical limitations: first, comprehensive data agent benchmarks remain absent due to the difficulty of designing test cases that evaluate agents' abilities across multi-source analytical tasks; second, constructing reliable test cases that combine structured and unstructured data remains costly and prohibitively complex; third, existing benchmarks exhibit limited adaptability and generalizability, resulting in narrow evaluation scope.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Databases},
  file = {/Users/yiyuanli/Zotero/storage/YGPP3TWX/Wang et al. - 2025 - FDABench A Benchmark for Data Agents on Analytical Queries over Heterogeneous Data.pdf}
}

@misc{wangFDABenchBenchmarkData2025a,
  title = {{{FDABench}}: {{A Benchmark}} for {{Data Agents}} on {{Analytical Queries}} over {{Heterogeneous Data}}},
  shorttitle = {{{FDABench}}},
  author = {Wang, Ziting and Zhang, Shize and Yuan, Haitao and Zhu, Jinwei and Li, Shifu and Dong, Wei and Cong, Gao},
  year = 2025,
  month = sep,
  number = {arXiv:2509.02473},
  eprint = {2509.02473},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2509.02473},
  urldate = {2025-09-12},
  abstract = {The growing demand for data-driven decision-making has created an urgent need for data agents that can integrate structured and unstructured data for analysis. While data agents show promise for enabling users to perform complex analytics tasks, this field still suffers from three critical limitations: first, comprehensive data agent benchmarks remain absent due to the difficulty of designing test cases that evaluate agents' abilities across multi-source analytical tasks; second, constructing reliable test cases that combine structured and unstructured data remains costly and prohibitively complex; third, existing benchmarks exhibit limited adaptability and generalizability, resulting in narrow evaluation scope.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Databases},
  file = {/Users/yiyuanli/Zotero/storage/NTBX2N38/Wang et al. - 2025 - FDABench A Benchmark for Data Agents on Analytical Queries over Heterogeneous Data.pdf}
}

@misc{wangGRPODAPOGSPO,
  title = {From {{GRPO}} to {{DAPO}} and {{GSPO}}: {{What}}, {{Why}}, and {{How}}},
  shorttitle = {From {{GRPO}} to {{DAPO}} and {{GSPO}}},
  author = {Wang, Yihua},
  urldate = {2025-08-28},
  abstract = {A Blog post by Yihua Zhang on Hugging Face},
  howpublished = {https://huggingface.co/blog/NormalUhr/grpo-to-dapo-and-gspo},
  file = {/Users/yiyuanli/Zotero/storage/XV3SXIPN/grpo-to-dapo-and-gspo.html}
}

@book{wangHuifengCiHua1975,
  title = {{Huifeng ci hua Ren jian ci hua}},
  author = {Wang, Zhongwen and {王仲聞} and {況周頤} and {王國維}},
  year = 1975,
  series = {{He luo wen ku}},
  number = {38},
  publisher = {He luo tu shu chu ban she},
  address = {Taibei},
  abstract = {Binder's title: Ren jian ci hua Huifeng ci hua., Reprint ed., Binder's title: 人間詞話蕙風詞話.},
  langid = {chi},
  keywords = {Ci (Chinese poetry),History and criticism}
}

@misc{wangMMLUProMoreRobust2024,
  title = {{{MMLU-Pro}}: {{A More Robust}} and {{Challenging Multi-Task Language Understanding Benchmark}}},
  shorttitle = {{{MMLU-Pro}}},
  author = {Wang, Yubo and Ma, Xueguang and Zhang, Ge and Ni, Yuansheng and Chandra, Abhranil and Guo, Shiguang and Ren, Weiming and Arulraj, Aaran and He, Xuan and Jiang, Ziyan and Li, Tianle and Ku, Max and Wang, Kai and Zhuang, Alex and Fan, Rongqi and Yue, Xiang and Chen, Wenhu},
  year = 2024,
  month = nov,
  number = {arXiv:2406.01574},
  eprint = {2406.01574},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.01574},
  urldate = {2025-09-29},
  abstract = {In the age of large-scale language models, benchmarks like the Massive Multitask Language Understanding (MMLU) have been pivotal in pushing the boundaries of what AI can achieve in language comprehension and reasoning across diverse domains. However, as models continue to improve, their performance on these benchmarks has begun to plateau, making it increasingly difficult to discern differences in model capabilities. This paper introduces MMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven MMLU benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options. Additionally, MMLU-Pro eliminates the trivial and noisy questions in MMLU. Our experimental results show that MMLU-Pro not only raises the challenge, causing a significant drop in accuracy by 16\% to 33\% compared to MMLU but also demonstrates greater stability under varying prompts. With 24 different prompt styles tested, the sensitivity of model scores to prompt variations decreased from 4-5\% in MMLU to just 2\% in MMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT) reasoning achieved better performance on MMLU-Pro compared to direct answering, which is in stark contrast to the findings on the original MMLU, indicating that MMLU-Pro includes more complex reasoning questions. Our assessments confirm that MMLU-Pro is a more discriminative benchmark to better track progress in the field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/yiyuanli/Zotero/storage/5ZVZFCVT/Wang et al. - 2024 - MMLU-Pro A More Robust and Challenging Multi-Task Language Understanding Benchmark.pdf;/Users/yiyuanli/Zotero/storage/8GQGVYAJ/2406.html}
}

@misc{wangSegmentAnythingSupervision2024,
  title = {Segment {{Anything}} without {{Supervision}}},
  author = {Wang, XuDong and Yang, Jingfeng and Darrell, Trevor},
  year = 2024,
  month = jun,
  number = {arXiv:2406.20081},
  eprint = {2406.20081},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.20081},
  urldate = {2025-09-28},
  abstract = {The Segmentation Anything Model (SAM) requires labor-intensive data labeling. We present Unsupervised SAM (UnSAM) for promptable and automatic wholeimage segmentation that does not require human annotations. UnSAM utilizes a divide-and-conquer strategy to ``discover'' the hierarchical structure of visual scenes. We first leverage top-down clustering methods to partition an unlabeled image into instance/semantic level segments. For all pixels within a segment, a bottom-up clustering method is employed to iteratively merge them into larger groups, thereby forming a hierarchical structure. These unsupervised multi-granular masks are then utilized to supervise model training. Evaluated across seven popular datasets, UnSAM achieves competitive results with the supervised counterpart SAM, and surpasses the previous state-of-the-art in unsupervised segmentation by 11\% in terms of AR. Moreover, we show that supervised SAM can also benefit from our self-supervised labels. By integrating our unsupervised pseudo masks into SA-1B's ground-truth masks and training UnSAM with only 1\% of SA-1B, a lightly semisupervised UnSAM can often segment entities overlooked by supervised SAM, exceeding SAM's AR by over 6.7\% and AP by 3.9\% on SA-1B.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/yiyuanli/Zotero/storage/3KATX43X/Wang et al. - 2024 - Segment Anything without Supervision.pdf}
}

@inproceedings{watsonSanzuDataScience2017,
  title = {Sanzu: {{A}} Data Science Benchmark},
  shorttitle = {Sanzu},
  booktitle = {2017 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Watson, Alex and Babu, Deepigha Shree Vittal and Ray, Suprio},
  year = 2017,
  month = dec,
  pages = {263--272},
  publisher = {IEEE},
  address = {Boston, MA},
  doi = {10.1109/BigData.2017.8257934},
  urldate = {2025-12-15},
  abstract = {The volume of data that is generated each day is rising rapidly. There is a need to analyze this data efficiently and produce results quickly. Data science offers a formal methodology for processing and analyzing data. It involves a work-flow with multiple stages, such as, data collection, data wrangling, statistical analysis and machine learning. In this paper, we look at data analytics systems that support the data science work-flow. The variety of current commercial and open-source data analytics systems differ significantly in terms of available features, functionality, and scalability. A benchmark can be used to evaluate the functionality and performance of a system. However, there is no standard benchmark for evaluating or comparing these data systems for doing data science. In this paper, we introduce a data science benchmark, Sanzu, to evaluate systems with data processing and analytics tasks. Our benchmark includes a micro and macro benchmark. The micro benchmark tests basic operations in isolation. It consists of task suites for reading and writing, data wrangling, statistical analysis, machine learning and time series analysis. Each macro workload evaluates an analytics application where a series of analysis or functions are based on a real world application. The macro benchmark focuses on sports and smart grid analytics. We evaluate these tasks on five different popular data science frameworks and systems: R, Anaconda Python, Dask, PostgreSQL (MADlib) and PySpark. For micro benchmark we generate synthetic datasets with 3 scale factors: 1, 10 and 100 (scale factor 1=1 million). The macro benchmark uses data generated from real-world data sources.},
  isbn = {978-1-5386-2715-0},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/TPJLXPTP/Watson et al. - 2017 - Sanzu A data science benchmark.pdf}
}

@article{weigertContentawareImageRestoration2018,
  title = {Content-Aware Image Restoration: Pushing the Limits of Fluorescence Microscopy},
  shorttitle = {Content-Aware Image Restoration},
  author = {Weigert, Martin and Schmidt, Uwe and Boothe, Tobias and M{\"u}ller, Andreas and Dibrov, Alexandr and Jain, Akanksha and Wilhelm, Benjamin and Schmidt, Deborah and Broaddus, Coleman and Culley, Si{\^a}n and {Rocha-Martins}, Mauricio and {Segovia-Miranda}, Fabi{\'a}n and Norden, Caren and Henriques, Ricardo and Zerial, Marino and Solimena, Michele and Rink, Jochen and Tomancak, Pavel and Royer, Loic and Jug, Florian and Myers, Eugene W.},
  year = 2018,
  month = dec,
  journal = {Nature Methods},
  volume = {15},
  number = {12},
  pages = {1090--1097},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/s41592-018-0216-7},
  urldate = {2025-10-08},
  abstract = {Fluorescence microscopy is a key driver of discoveries in the life sciences, with observable phenomena being limited by the optics of the microscope, the chemistry of the fluorophores, and the maximum photon exposure tolerated by the sample. These limits necessitate trade-offs between imaging speed, spatial resolution, light exposure, and imaging depth. In this work we show how content-aware image restoration based on deep learning extends the range of biological phenomena observable by microscopy. We demonstrate on eight concrete examples how microscopy images can be restored even if 60-fold fewer photons are used during acquisition, how near isotropic resolution can be achieved with up to tenfold under-sampling along the axial direction, and how tubular and granular structures smaller than the diffraction limit can be resolved at 20-times-higher frame rates compared to state-of-the-art methods. All developed image restoration methods are freely available as open source software in Python, FIJI, and KNIME.},
  copyright = {2018 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Image processing,Machine learning,Microscopy,Software},
  file = {/Users/yiyuanli/Zotero/storage/ISBTD544/Weigert et al. - 2018 - Content-aware image restoration pushing the limits of fluorescence microscopy.pdf}
}

@inproceedings{weigertNucleiInstanceSegmentation2022,
  title = {Nuclei Instance Segmentation and Classification in Histopathology Images with {{StarDist}}},
  booktitle = {2022 {{IEEE International Symposium}} on {{Biomedical Imaging Challenges}} ({{ISBIC}})},
  author = {Weigert, Martin and Schmidt, Uwe},
  year = 2022,
  month = mar,
  eprint = {2203.02284},
  primaryclass = {cs},
  pages = {1--4},
  doi = {10.1109/ISBIC56247.2022.9854534},
  urldate = {2025-10-08},
  abstract = {Instance segmentation and classification of nuclei is an important task in computational pathology. We show that StarDist, a deep learning nuclei segmentation method originally developed for fluorescence microscopy, can be extended and successfully applied to histopathology images. This is substantiated by conducting experiments on the Lizard dataset, and through entering the Colon Nuclei Identification and Counting (CoNIC) challenge 2022, where our approach achieved the first spot on the leaderboard for the segmentation and classification task for both the preliminary and final test phase.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/yiyuanli/Zotero/storage/P8P9TETC/Weigert and Schmidt - 2022 - Nuclei instance segmentation and classification in histopathology images with StarDist.pdf}
}

@misc{wenReinforcementLearningVerifiable2025,
  title = {Reinforcement {{Learning}} with {{Verifiable Rewards Implicitly Incentivizes Correct Reasoning}} in {{Base LLMs}}},
  author = {Wen, Xumeng and Liu, Zihan and Zheng, Shun and Xu, Zhijian and Ye, Shengyu and Wu, Zhirong and Liang, Xiao and Wang, Yang and Li, Junjie and Miao, Ziming and Bian, Jiang and Yang, Mao},
  year = 2025,
  month = jun,
  number = {arXiv:2506.14245},
  eprint = {2506.14245},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.14245},
  urldate = {2025-08-28},
  abstract = {Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a promising paradigm for advancing the reasoning capabilities of Large Language Models (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned models often underperform their base models on the P ass@K metric for solutionfinding, leading to the hypothesis that RLVR merely re-weights existing reasoning paths at the cost of reasoning diversity. In this work, we resolve this contradiction by identifying the source of the problem: the P ass@K metric itself is a flawed measure of reasoning, as it credits correct final answers that probably arise from inaccurate or incomplete chains of thought (CoTs). To address this, we introduce a more precise evaluation metric, CoT -P ass@K, which mandates that both the reasoning path and the final answer be correct. We provide a new theoretical foundation that formalizes how RLVR, unlike traditional RL, is uniquely structured to incentivize logical integrity. Our empirical results are supportive: using CoT P ass@K, we observe that RLVR can incentivize the generalization of correct reasoning for all values of K. Furthermore, by analyzing the training dynamics, we find that this enhanced reasoning capability emerges early in the training process and smoothly generalizes. Our work provides a clear perspective on the role of RLVR, offers a more reliable method for its evaluation, and confirms its potential to genuinely advance machine reasoning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/yiyuanli/Zotero/storage/8IQ5HP5W/Wen et al. - 2025 - Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLM.pdf}
}

@article{xingMMTUMassiveMultiTask,
  title = {{{MMTU}}: {{A Massive Multi-Task Table Understanding}} and {{Reasoning Benchmark}}},
  author = {Xing, Junjie and He, Yeye and Zhou, Mengyu and Dong, Haoyu and Han, Shi and Chen, Lingjiao and Zhang, Dongmei and Chaudhuri, Surajit and Jagadish, H V},
  abstract = {Tables and table-based use cases play a crucial role in many important real-world applications, such as spreadsheets, databases, and computational notebooks, which traditionally require expert-level users like data engineers, data analysts, and database administrators to operate. Although LLMs have shown remarkable progress in working with tables (e.g., in spreadsheet and database copilot scenarios), comprehensive benchmarking of such capabilities remains limited. In contrast to an extensive and growing list of NLP benchmarks, evaluations of table-related tasks are scarce, and narrowly focus on tasks like NL-to-SQL and Table-QA, overlooking the broader spectrum of real-world tasks that professional users face. This gap limits our understanding and model progress in this important area.},
  langid = {english},
  file = {/Users/yiyuanli/Zotero/storage/IFI6EBBG/Xing et al. - MMTU A Massive Multi-Task Table Understanding and Reasoning Benchmark.pdf}
}

@misc{xuQwen3OmniTechnicalReport2025,
  title = {Qwen3-{{Omni Technical Report}}},
  author = {Xu, Jin and Guo, Zhifang and Hu, Hangrui and Chu, Yunfei and Wang, Xiong and He, Jinzheng and Wang, Yuxuan and Shi, Xian and He, Ting and Zhu, Xinfa and Lv, Yuanjun and Wang, Yongqi and Guo, Dake and Wang, He and Ma, Linhan and Zhang, Pei and Zhang, Xinyu and Hao, Hongkun and Guo, Zishan and Yang, Baosong and Zhang, Bin and Ma, Ziyang and Wei, Xipin and Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Peng and Yang, Mingkun and Liu, Dayiheng and Ren, Xingzhang and Zheng, Bo and Men, Rui and Zhou, Fan and Yu, Bowen and Yang, Jianxin and Yu, Le and Zhou, Jingren and Lin, Junyang},
  year = 2025,
  month = sep,
  number = {arXiv:2509.17765},
  eprint = {2509.17765},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2509.17765},
  urldate = {2025-09-27},
  abstract = {We present Qwen3-Omni, a single multimodal model that for the first time maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves opensource state-of-the-art (SOTA) on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker--Talker Mixture-of-Experts (MoE) architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages and speech generation in 10 languages. The system can process audio recordings up to 40 minutes per instance for ASR and spoken-language understanding, enabling high-quality audio and audiovisual experiences across locales. It demonstrates strong instruction following and allows fine-grained customization of conversational tone and persona via user-defined system prompts. To reduce first-packet latency in streaming synthesis, the Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings (no prior context), Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms. To further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/Users/yiyuanli/Zotero/storage/HH7YYL7U/Xu et al. - 2025 - Qwen3-Omni Technical Report.pdf}
}

@misc{xuRMPSAMRealTimeMultiPurpose2025,
  title = {{{RMP-SAM}}: {{Towards Real-Time Multi-Purpose Segment Anything}}},
  shorttitle = {{{RMP-SAM}}},
  author = {Xu, Shilin and Yuan, Haobo and Shi, Qingyu and Qi, Lu and Wang, Jingbo and Yang, Yibo and Li, Yining and Chen, Kai and Tong, Yunhai and Ghanem, Bernard and Li, Xiangtai and Yang, Ming-Hsuan},
  year = 2025,
  month = feb,
  number = {arXiv:2401.10228},
  eprint = {2401.10228},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.10228},
  urldate = {2025-09-28},
  abstract = {Recent segmentation methods, which adopt large-scale data training and transformer architecture, aim to create one foundation model that can perform multiple tasks. However, most of these methods rely on heavy encoder and decoder frameworks, hindering their performance in real-time scenarios. To explore real-time segmentation, recent advancements primarily focus on semantic segmentation within specific environments, such as autonomous driving. However, they often overlook the generalization ability of these models across diverse scenarios. Therefore, to fill this gap, this work explores a novel real-time segmentation setting called real-time multi-purpose segmentation. It contains three fundamental sub-tasks: interactive segmentation, panoptic segmentation, and video instance segmentation. Unlike previous methods, which use a specific design for each task, we aim to use only a single end-to-end model to accomplish all these tasks in real-time. To meet real-time requirements and balance multi-task learning, we present a novel dynamic convolution-based method, Real-Time Multi-Purpose SAM (RMP-SAM). It contains an efficient encoder and an efficient decoupled adapter to perform prompt-driven decoding. Moreover, we further explore different training strategies and one new adapter design to boost co-training performance further. We benchmark several strong baselines by extending existing works to support our multi-purpose segmentation. Extensive experiments demonstrate that RMP-SAM is effective and generalizes well on proposed benchmarks and other specific semantic tasks. Our implementation of RMP-SAM achieves the optimal balance between accuracy and speed for these tasks.Our code and model are available at https://github.com/xushilin1/RAP-SAM/.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/yiyuanli/Zotero/storage/I5NL9RMQ/Xu et al. - 2025 - RMP-SAM Towards Real-Time Multi-Purpose Segment Anything.pdf}
}

@misc{yangCCOCRComprehensiveChallenging2024a,
  title = {{{CC-OCR}}: {{A Comprehensive}} and {{Challenging OCR Benchmark}} for {{Evaluating Large Multimodal Models}} in {{Literacy}}},
  shorttitle = {{{CC-OCR}}},
  author = {Yang, Zhibo and Tang, Jun and Li, Zhaohai and Wang, Pengfei and Wan, Jianqiang and Zhong, Humen and Liu, Xuejing and Yang, Mingkun and Wang, Peng and Bai, Shuai and Jin, LianWen and Lin, Junyang},
  year = 2024,
  month = dec,
  number = {arXiv:2412.02210},
  eprint = {2412.02210},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.02210},
  urldate = {2025-09-23},
  abstract = {Large Multimodal Models (LMMs) have demonstrated impressive performance in recognizing document images with natural language instructions. However, it remains unclear to what extent capabilities in literacy with rich structure and fine-grained visual challenges. The current landscape lacks a comprehensive benchmark to effectively measure the literate capabilities of LMMs. Existing benchmarks are often limited by narrow scenarios and specified tasks. To this end, we introduce CC-OCR, a comprehensive benchmark that possesses a diverse range of scenarios, tasks, and challenges. CC-OCR comprises four OCR-centric tracks: multi-scene text reading, multilingual text reading, document parsing, and key information extraction. It includes 39 subsets with 7,058 full annotated images, of which 41\% are sourced from real applications, and released for the first time. We evaluate nine prominent LMMs and reveal both the strengths and weaknesses of these models, particularly in text grounding, multi-orientation, and hallucination of repetition. CC-OCR aims to comprehensively evaluate the capabilities of LMMs on OCR-centered tasks, facilitating continued progress in this crucial area.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/yiyuanli/Zotero/storage/3YG7JEJJ/Yang et al. - 2024 - CC-OCR A Comprehensive and Challenging OCR Benchmark for Evaluating Large Multimodal Models in Lite.pdf}
}

@misc{yaoSecondHalf,
  title = {The {{Second Half}}},
  author = {Yao, Shunyu},
  urldate = {2025-09-01},
  abstract = {tldr: We're at AI's halftime.},
  file = {/Users/yiyuanli/Zotero/storage/VY3HWJ8L/The-Second-Half.html}
}

@misc{YichunYiyuanPhilosophy,
  title = {Yichun and {{Yiyuan Philosophy}} \textbar{} {{Zotero}}},
  urldate = {2023-10-23},
  howpublished = {https://www.zotero.org/groups/4897948/yichun\_and\_yiyuan\_philosophy/collections/WFAB35S8},
  file = {/Users/yiyuanli/Zotero/storage/TJI4KCQU/WFAB35S8.html}
}

@inproceedings{yoonLanguageOCRForm2024,
  title = {Language, {{OCR}}, {{Form Independent}} ({{LOFI}}) Pipeline for {{Industrial Document Information Extraction}}},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}: {{Industry Track}}},
  author = {Yoon, Chang Oh and Lee, Wonbeen and Jang, Seokhwan and Choi, Kyuwon and Jung, Minsung and Choi, Daewoo},
  editor = {Dernoncourt, Franck and {Preo{\c t}iuc-Pietro}, Daniel and Shimorina, Anastasia},
  year = 2024,
  month = nov,
  pages = {1056--1067},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, US},
  doi = {10.18653/v1/2024.emnlp-industry.79},
  urldate = {2024-12-16},
  abstract = {This paper presents LOFI (Language, OCR, Form Independent), a pipeline for Document Information Extraction (DIE) in Low-Resource Language (LRL) business documents. LOFI pipeline solves language, Optical Character Recognition (OCR), and form dependencies through flexible model architecture, a token-level box split algorithm, and the SPADE decoder. Experiments on Korean and Japanese documents demonstrate high performance in Semantic Entity Recognition (SER) task without additional pre-training. The pipeline's effectiveness is validated through real-world applications in insurance and tax-free declaration services, advancing DIE capabilities for diverse languages and document types in industrial settings.},
  file = {/Users/yiyuanli/Zotero/storage/9DPRUAUF/Yoon et al. - 2024 - Language, OCR, Form Independent (LOFI) pipeline fo.pdf}
}

@misc{yuDAPOOpenSourceLLM2025,
  title = {{{DAPO}}: {{An Open-Source LLM Reinforcement Learning System}} at {{Scale}}},
  shorttitle = {{{DAPO}}},
  author = {Yu, Qiying and Zhang, Zheng and Zhu, Ruofei and Yuan, Yufeng and Zuo, Xiaochen and Yue, Yu and Dai, Weinan and Fan, Tiantian and Liu, Gaohong and Liu, Lingjun and Liu, Xin and Lin, Haibin and Lin, Zhiqi and Ma, Bole and Sheng, Guangming and Tong, Yuxuan and Zhang, Chi and Zhang, Mofan and Zhang, Wang and Zhu, Hang and Zhu, Jinhua and Chen, Jiaze and Chen, Jiangjie and Wang, Chengyi and Yu, Hongli and Song, Yuxuan and Wei, Xiangpeng and Zhou, Hao and Liu, Jingjing and Ma, Wei-Ying and Zhang, Ya-Qin and Yan, Lin and Qiao, Mu and Wu, Yonghui and Wang, Mingxuan},
  year = 2025,
  month = may,
  number = {arXiv:2503.14476},
  eprint = {2503.14476},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.14476},
  urldate = {2025-08-28},
  abstract = {Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the Decoupled Clip and Dynamic sAmpling Policy Optimization (DAPO) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training code, which is built on the verl framework a, along with a carefully curated and processed dataset. These components of our open-source system enhance reproducibility and support future research in large-scale LLM RL.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/yiyuanli/Zotero/storage/7Q7Q8JC4/Yu et al. - 2025 - DAPO An Open-Source LLM Reinforcement Learning System at Scale.pdf}
}

@misc{yueMMMUMassiveMultidiscipline2024,
  title = {{{MMMU}}: {{A Massive Multi-discipline Multimodal Understanding}} and {{Reasoning Benchmark}} for {{Expert AGI}}},
  shorttitle = {{{MMMU}}},
  author = {Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and Wei, Cong and Yu, Botao and Yuan, Ruibin and Sun, Renliang and Yin, Ming and Zheng, Boyuan and Yang, Zhenzhu and Liu, Yibo and Huang, Wenhao and Sun, Huan and Su, Yu and Chen, Wenhu},
  year = 2024,
  month = jun,
  number = {arXiv:2311.16502},
  eprint = {2311.16502},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.16502},
  urldate = {2025-09-29},
  abstract = {We introduce MMMU: a new benchmark designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning. MMMU includes 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art \& Design, Business, Science, Health \& Medicine, Humanities \& Social Science, and Tech \& Engineering. These questions span 30 subjects and 183 subfields, comprising 30 highly heterogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures. Unlike existing benchmarks, MMMU focuses on advanced perception and reasoning with domain-specific knowledge, challenging models to perform tasks akin to those faced by experts. The evaluation of 14 open-source LMMs as well as the proprietary GPT-4V(ision) and Gemini highlights the substantial challenges posed by MMMU. Even the advanced GPT-4V and Gemini Ultra only achieve accuracies of 56\% and 59\% respectively, indicating significant room for improvement. We believe MMMU will stimulate the community to build next-generation multimodal foundation models towards expert artificial general intelligence.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/yiyuanli/Zotero/storage/K65ZF5WI/Yue et al. - 2024 - MMMU A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI.pdf}
}

@misc{yuHongLouMeng1980,
  title = {{Hong lou meng shi ci lian yu ping zhu}},
  author = {Yu, Zhou and {于舟}},
  year = 1980,
  journal = {Hong lou meng shi ci lian yu ping zhu},
  edition = {Di 1 ban., 第1版.},
  publisher = {Shanxi jiao yu chu ban she},
  address = {Taiyuan},
  collaborator = {Niu, Wu and {牛武} and {曹雪芹}},
  isbn = {9787805785103},
  langid = {chi},
  keywords = {Cao Xueqin approximately 1717-1763 Hong lou meng,Cao Xueqin approximately 1717-1763 Poetic works,Chinese poetry,History and criticism,Qing dynasty 1644-1912}
}

@misc{zhangBenchmarkingDataScience2024,
  title = {Benchmarking {{Data Science Agents}}},
  author = {Zhang, Yuge and Jiang, Qiyang and Han, Xingyu and Chen, Nan and Yang, Yuqing and Ren, Kan},
  year = 2024,
  month = feb,
  number = {arXiv:2402.17168},
  eprint = {2402.17168},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2402.17168},
  urldate = {2025-12-15},
  abstract = {In the era of data-driven decision-making, the complexity of data analysis necessitates advanced expertise and tools of data science, presenting significant challenges even for specialists. Large Language Models (LLMs) have emerged as promising aids as data science agents, assisting humans in data analysis and processing. Yet their practical efficacy remains constrained by the varied demands of real-world applications and complicated analytical process. In this paper, we introduce DSEval -- a novel evaluation paradigm, as well as a series of innovative benchmarks tailored for assessing the performance of these agents throughout the entire data science lifecycle. Incorporating a novel bootstrapped annotation method, we streamline dataset preparation, improve the evaluation coverage, and expand benchmarking comprehensiveness. Our findings uncover prevalent obstacles and provide critical insights to inform future advancements in the field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/yiyuanli/Zotero/storage/HPCPXP6W/Zhang et al. - 2024 - Benchmarking Data Science Agents.pdf;/Users/yiyuanli/Zotero/storage/797B84ME/2402.html}
}

@book{zhangCiYuan1968,
  title = {{Ci yuan}},
  author = {Zhang, Yan and {張炎}},
  year = 1968,
  series = {{Guo xue ji ben cong shu si bai zhong}},
  edition = {Tai 1 ban., 臺1版.},
  number = {217},
  publisher = {Taiwan shang wu yin shu guan},
  address = {Taibei Shi},
  abstract = {Series statement: Guo xue ji ben cong shu si bai zhong ; 217},
  langid = {chi},
  keywords = {Ci (Chinese poetry),History and criticism,Poetics}
}

@misc{zhangDataSciBenchLLMAgent2025,
  title = {{{DataSciBench}}: {{An LLM Agent Benchmark}} for {{Data Science}}},
  shorttitle = {{{DataSciBench}}},
  author = {Zhang, Dan and Zhoubian, Sining and Cai, Min and Li, Fengzu and Yang, Lekang and Wang, Wei and Dong, Tianjiao and Hu, Ziniu and Tang, Jie and Yue, Yisong},
  year = 2025,
  month = feb,
  number = {arXiv:2502.13897},
  eprint = {2502.13897},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.13897},
  urldate = {2025-09-02},
  abstract = {This paper presents DataSciBench, a comprehensive benchmark for evaluating Large Language Model (LLM) capabilities in data science. Recent related benchmarks have primarily focused on single tasks, easily obtainable ground truth, and straightforward evaluation metrics, which limits the scope of tasks that can be evaluated. In contrast, DataSciBench is constructed based on a more comprehensive and curated collection of natural and challenging prompts for uncertain ground truth and evaluation metrics. We develop a semi-automated pipeline for generating ground truth (GT) and validating evaluation metrics. This pipeline utilizes and implements an LLM-based self-consistency and human verification strategy to produce accurate GT by leveraging collected prompts, predefined task types, and aggregate functions (metrics). Furthermore, we propose an innovative Task - Function - Code (TFC) framework to assess each code execution outcome based on precisely defined metrics and programmatic rules. Our experimental framework involves testing 6 API-based models, 8 open-source general models, and 9 opensource code generation models using the diverse set of prompts we have gathered. This approach aims to provide a more comprehensive and rigorous evaluation of LLMs in data science, revealing their strengths and weaknesses. Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder33B-Instruct achieves the highest score among open-sourced models. We release all code and data at https://github.com/THUDM/ DataSciBench/.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/yiyuanli/Zotero/storage/UXRNHRNJ/Zhang et al. - 2025 - DataSciBench An LLM Agent Benchmark for Data Science.pdf}
}

@misc{zhangDataSciBenchLLMAgent2025a,
  title = {{{DataSciBench}}: {{An LLM Agent Benchmark}} for {{Data Science}}},
  shorttitle = {{{DataSciBench}}},
  author = {Zhang, Dan and Zhoubian, Sining and Cai, Min and Li, Fengzu and Yang, Lekang and Wang, Wei and Dong, Tianjiao and Hu, Ziniu and Tang, Jie and Yue, Yisong},
  year = 2025,
  month = feb,
  number = {arXiv:2502.13897},
  eprint = {2502.13897},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.13897},
  urldate = {2025-12-15},
  abstract = {This paper presents DataSciBench, a comprehensive benchmark for evaluating Large Language Model (LLM) capabilities in data science. Recent related benchmarks have primarily focused on single tasks, easily obtainable ground truth, and straightforward evaluation metrics, which limits the scope of tasks that can be evaluated. In contrast, DataSciBench is constructed based on a more comprehensive and curated collection of natural and challenging prompts for uncertain ground truth and evaluation metrics. We develop a semi-automated pipeline for generating ground truth (GT) and validating evaluation metrics. This pipeline utilizes and implements an LLM-based self-consistency and human verification strategy to produce accurate GT by leveraging collected prompts, predefined task types, and aggregate functions (metrics). Furthermore, we propose an innovative Task - Function - Code (TFC) framework to assess each code execution outcome based on precisely defined metrics and programmatic rules. Our experimental framework involves testing 6 API-based models, 8 open-source general models, and 9 open-source code generation models using the diverse set of prompts we have gathered. This approach aims to provide a more comprehensive and rigorous evaluation of LLMs in data science, revealing their strengths and weaknesses. Experimental results demonstrate that API-based models outperform open-sourced models on all metrics and Deepseek-Coder-33B-Instruct achieves the highest score among open-sourced models. We release all code and data at https://github.com/THUDM/DataSciBench.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/yiyuanli/Zotero/storage/5XUES9KQ/Zhang et al. - 2025 - DataSciBench An LLM Agent Benchmark for Data Science.pdf;/Users/yiyuanli/Zotero/storage/QQA484BP/2502.html}
}

@inproceedings{zhangSciEREntityRelation2024,
  title = {{{SciER}}: {{An Entity}} and {{Relation Extraction Dataset}} for {{Datasets}}, {{Methods}}, and {{Tasks}} in {{Scientific Documents}}},
  shorttitle = {{{SciER}}},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Zhang, Qi and Chen, Zhijia and Pan, Huitong and Caragea, Cornelia and Latecki, Longin Jan and Dragut, Eduard},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = 2024,
  month = nov,
  pages = {13083--13100},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.726},
  urldate = {2024-12-16},
  abstract = {Scientific information extraction (SciIE) is critical for converting unstructured knowledge from scholarly articles into structured data (entities and relations). Several datasets have been proposed for training and validating SciIE models. However, due to the high complexity and cost of annotating scientific texts, those datasets restrict their annotations to specific parts of paper, such as abstracts, resulting in the loss of diverse entity mentions and relations in context. In this paper, we release a new entity and relation extraction dataset for entities related to datasets, methods, and tasks in scientific articles. Our dataset contains 106 manually annotated full-text scientific publications with over 24k entities and 12k relations. To capture the intricate use and interactions among entities in full texts, our dataset contains a fine-grained tag set for relations. Additionally, we provide an out-of-distribution test set to offer a more realistic evaluation. We conduct comprehensive experiments, including state-of-the-art supervised models and our proposed LLM-based baselines, and highlight the challenges presented by our dataset, encouraging the development of innovative models to further the field of SciIE.},
  file = {/Users/yiyuanli/Zotero/storage/3J9X6IZV/Zhang et al. - 2024 - SciER An Entity and Relation Extraction Dataset f.pdf}
}

@inproceedings{zhangSRFEnhancingDocumentLevel2024,
  title = {{{SRF}}: {{Enhancing Document-Level Relation Extraction}} with a {{Novel Secondary Reasoning Framework}}},
  shorttitle = {{{SRF}}},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Zhang, Fu and Miao, Qi and Cheng, Jingwei and Yu, Hongsen and Yan, Yi and Li, Xin and Wu, Yongxue},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = 2024,
  month = nov,
  pages = {15426--15439},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.863},
  urldate = {2024-12-16},
  abstract = {Document-level Relation Extraction (DocRE) aims to extract relations between entity pairs in a document and poses many challenges as it involves multiple mentions of entities and cross-sentence inference. However, several aspects that are important for DocRE have not been considered and explored. Existing work ignores bidirectional mention interaction when generating relational features for entity pairs. Also, sophisticated neural networks are typically designed for cross-sentence evidence extraction to further enhance DocRE. More interestingly, we reveal a noteworthy finding: If a model has predicted a relation between an entity and other entities, this relation information may help infer and predict more relations between the entity's adjacent entities and these other entities. Nonetheless, none of existing methods leverage secondary reasoning to exploit results of relation prediction. To this end, we propose a novel Secondary Reasoning Framework (SRF) for DocRE. In SRF, we initially propose a DocRE model that incorporates bidirectional mention fusion and a simple yet effective evidence extraction module (incurring only an additional learnable parameter overhead) for relation prediction. Further, for the first time, we elaborately design and propose a novel secondary reasoning method to discover more relations by exploring the results of the first relation prediction. Extensive experiments show that SRF achieves SOTA performance and our secondary reasoning method is both effective and general when integrated into existing models.},
  file = {/Users/yiyuanli/Zotero/storage/MW8M5YU8/Zhang et al. - 2024 - SRF Enhancing Document-Level Relation Extraction .pdf}
}

@misc{zhangSurveyReinforcementLearning2025,
  title = {A {{Survey}} of {{Reinforcement Learning}} for {{Large Reasoning Models}}},
  author = {Zhang, Kaiyan and Zuo, Yuxin and He, Bingxiang and Sun, Youbang and Liu, Runze and Jiang, Che and Fan, Yuchen and Tian, Kai and Jia, Guoli and Li, Pengfei and Fu, Yu and Lv, Xingtai and Zhang, Yuchen and Zeng, Sihang and Qu, Shang and Li, Haozhan and Wang, Shijie and Wang, Yuru and Long, Xinwei and Liu, Fangfu and Xu, Xiang and Ma, Jiaze and Zhu, Xuekai and Hua, Ermo and Liu, Yihao and Li, Zonglin and Chen, Huayu and Qu, Xiaoye and Li, Yafu and Chen, Weize and Yuan, Zhenzhao and Gao, Junqi and Li, Dong and Ma, Zhiyuan and Cui, Ganqu and Liu, Zhiyuan and Qi, Biqing and Ding, Ning and Zhou, Bowen},
  year = 2025,
  month = sep,
  number = {arXiv:2509.08827},
  eprint = {2509.08827},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2509.08827},
  urldate = {2025-09-24},
  abstract = {In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/yiyuanli/Zotero/storage/KHPQVD3Y/Zhang et al. - 2025 - A Survey of Reinforcement Learning for Large Reasoning Models.pdf}
}

@misc{zhangSurveyReinforcementLearning2025a,
  title = {A {{Survey}} of {{Reinforcement Learning}} for {{Large Reasoning Models}}},
  author = {Zhang, Kaiyan and Zuo, Yuxin and He, Bingxiang and Sun, Youbang and Liu, Runze and Jiang, Che and Fan, Yuchen and Tian, Kai and Jia, Guoli and Li, Pengfei and Fu, Yu and Lv, Xingtai and Zhang, Yuchen and Zeng, Sihang and Qu, Shang and Li, Haozhan and Wang, Shijie and Wang, Yuru and Long, Xinwei and Liu, Fangfu and Xu, Xiang and Ma, Jiaze and Zhu, Xuekai and Hua, Ermo and Liu, Yihao and Li, Zonglin and Chen, Huayu and Qu, Xiaoye and Li, Yafu and Chen, Weize and Yuan, Zhenzhao and Gao, Junqi and Li, Dong and Ma, Zhiyuan and Cui, Ganqu and Liu, Zhiyuan and Qi, Biqing and Ding, Ning and Zhou, Bowen},
  year = 2025,
  month = sep,
  number = {arXiv:2509.08827},
  eprint = {2509.08827},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2509.08827},
  urldate = {2025-09-27},
  abstract = {In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/yiyuanli/Zotero/storage/34SQHERF/Zhang et al. - 2025 - A Survey of Reinforcement Learning for Large Reasoning Models.pdf}
}

@misc{zhaoSurveyLargeLanguage2025,
  title = {A {{Survey}} of {{Large Language Models}}},
  author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
  year = 2025,
  month = mar,
  number = {arXiv:2303.18223},
  eprint = {2303.18223},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.18223},
  urldate = {2025-12-18},
  abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/yiyuanli/Zotero/storage/F9BT4X96/Zhao et al. - 2025 - A Survey of Large Language Models.pdf;/Users/yiyuanli/Zotero/storage/Y7HX7BNK/2303.html}
}

@misc{zhengGroupSequencePolicy2025,
  title = {Group {{Sequence Policy Optimization}}},
  author = {Zheng, Chujie and Liu, Shixuan and Li, Mingze and Chen, Xiong-Hui and Yu, Bowen and Gao, Chang and Dang, Kai and Liu, Yuqiong and Men, Rui and Yang, An and Zhou, Jingren and Lin, Junyang},
  year = 2025,
  month = jul,
  number = {arXiv:2507.18071},
  eprint = {2507.18071},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2507.18071},
  urldate = {2025-08-28},
  abstract = {This paper introduces Group Sequence Policy Optimization (GSPO), our stable, efficient, and performant reinforcement learning algorithm for training large language models. Unlike previous algorithms that adopt token-level importance ratios, GSPO defines the importance ratio based on sequence likelihood and performs sequence-level clipping, rewarding, and optimization. We demonstrate that GSPO achieves superior training efficiency and performance compared to the GRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and has the potential for simplifying the design of RL infrastructure. These merits of GSPO have contributed to the remarkable improvements in the latest Qwen3 models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/yiyuanli/Zotero/storage/BQDNKCN3/Zheng et al. - 2025 - Group Sequence Policy Optimization.pdf}
}

@inproceedings{zhouGraspingEssentialsTailoring2024,
  title = {Grasping the {{Essentials}}: {{Tailoring Large Language Models}} for {{Zero-Shot Relation Extraction}}},
  shorttitle = {Grasping the {{Essentials}}},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Zhou, Sizhe and Meng, Yu and Jin, Bowen and Han, Jiawei},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = 2024,
  month = nov,
  pages = {13462--13486},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.747},
  urldate = {2024-12-16},
  abstract = {Relation extraction (RE) aims to identify semantic relationships between entities within text. Despite considerable advancements, existing models predominantly require extensive annotated training data, which is both costly and labor-intensive to collect. Moreover, these models often struggle to adapt to new or unseen relations. Few-shot learning, aiming to lessen annotation demands, typically provides incomplete and biased supervision for target relations, leading to degraded and unstable performance. To accurately and explicitly describe relation semantics while minimizing annotation demands, we explore the definition only zero-shot RE setting where only relation definitions expressed in natural language are used to train a RE model. We introduce REPaL, comprising three stages: (1) We leverage large language models (LLMs) to generate initial seed instances from relation definitions and an unlabeled corpus. (2) We fine-tune a bidirectional Small Language Model (SLM) with initial seeds to learn relations for the target domain. (3) We expand pattern coverage and mitigate bias from initial seeds by integrating feedback from the SLM's predictions on the unlabeled corpus and the synthesis history. To accomplish this, we leverage the multi-turn conversation ability of LLMs to generate new instances in follow-up dialogues, informed by both the feedback and synthesis history. Studies reveal that definition-oriented seed synthesis enhances pattern coverage whereas indiscriminately increasing seed quantity leads to performance saturation. Experiments on two datasets show REPaL significantly improved cost-effective zero-shot performance by large margins.},
  file = {/Users/yiyuanli/Zotero/storage/ZTL9V957/Zhou et al. - 2024 - Grasping the Essentials Tailoring Large Language .pdf}
}

@misc{zhuAutoLogiAutomatedGeneration2025,
  title = {{{AutoLogi}}: {{Automated Generation}} of {{Logic Puzzles}} for {{Evaluating Reasoning Abilities}} of {{Large Language Models}}},
  shorttitle = {{{AutoLogi}}},
  author = {Zhu, Qin and Huang, Fei and Peng, Runyu and Lu, Keming and Yu, Bowen and Cheng, Qinyuan and Qiu, Xipeng and Huang, Xuanjing and Lin, Junyang},
  year = 2025,
  month = feb,
  number = {arXiv:2502.16906},
  eprint = {2502.16906},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.16906},
  urldate = {2025-09-27},
  abstract = {While logical reasoning evaluation of Large Language Models (LLMs) has attracted significant attention, existing benchmarks predominantly rely on multiple-choice formats that are vulnerable to random guessing, leading to overestimated performance and substantial performance fluctuations. To obtain more accurate assessments of models' reasoning capabilities, we propose an automated method for synthesizing open-ended logic puzzles, and use it to develop a bilingual benchmark, AutoLogi. Our approach features program-based verification and controllable difficulty levels, enabling more reliable evaluation that better distinguishes models' reasoning abilities. Extensive evaluation of eight modern LLMs shows that AutoLogi can better reflect true model capabilities, with performance scores spanning from 35\% to 73\% compared to the narrower range of 21\% to 37\% on the source multiplechoice dataset. Beyond benchmark creation, this synthesis method can generate high-quality training data by incorporating program verifiers into the rejection sampling process, enabling systematic enhancement of LLMs' reasoning capabilities across diverse datasets.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/yiyuanli/Zotero/storage/U7WPP6EH/Zhu et al. - 2025 - AutoLogi Automated Generation of Logic Puzzles for Evaluating Reasoning Abilities of Large Language.pdf}
}

@misc{ZoteroYourPersonal,
  title = {Zotero \textbar{} {{Your}} Personal Research Assistant},
  urldate = {2025-08-28},
  howpublished = {https://www.zotero.org/start}
}
